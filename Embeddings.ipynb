{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Espinoza_TP_Word_Embeddings_IA312.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"Tfiyv4C9HA20","colab":{}},"source":["# Contrôle de version\n","import sys\n","assert sys.version_info[0]==3\n","assert sys.version_info[1] >= 5\n","\n","# Packages nécessaires\n","import numpy as np\n","import os\n","import re\n","import matplotlib.pyplot as plt\n","# A la première utilisation de nltk, télécharger les données nécessaires\n","import nltk\n","#nltk.download('punkt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hWJ8EZ67HA25"},"source":["[//]: # (<span style=\"color:red\">TODO</span> pour signaler du contenu manquant)\n","\n","##  Word Embeddings : Représentations distribuées via l'hypothèse distributionelle\n","\n","**But**: On va chercher à obtenir des représentations denses (comme vecteurs de nombres réels) de mots (et éventuellement de phrases). Ces représentations ont vocation à être distribuées: ce sont des représentations non-locales. On représente un objet comme une combinaison de *features*, par opposition à l'attribution d'un symbole dédié: voir le travail fondateur d'entre autres, Geoffrey Hinton, sur le sujet: [Distributed Representations](https://web.stanford.edu/~jlmcc/papers/PDP/Chapter3.pdf).\n","\n","Le terme de représentation *distribuées* est très général, mais correspond à que l'on cherche à obtenir. L'enjeu est donc de pouvoir construire, automatiquement, de telles représentations.\n","\n","**Idée sous-jacente**: Elle est basée sur l'hypothèse distributionelle: les informations contextuelles suffisent à obtenir une représentation viable d'objets linguistiques.\n"," - *“For a large class of cases [...] the  meaning  of a word is  its  use in the  language.”* Wittgenstein (Philosophical Investigations, 43 - 1953)\n"," - *“You shall know a word by the company it keeps”*, Firth (\"A synopsis of linguistic theory 1930-1955.\" - 1957)\n","\n","Ainsi, on peut caractériser un mot par les mots qui l'accompagnent, via des comptes de co-occurences. Deux mots ayant un sens similaire auront une distribution contextuelle similaire et auront donc plus de chance d'apparaître dans des contextes similaires. Cette hypothèse peut servir de justification à l'application de statistiques à la sémantique (extraction d'information, analyse sémantique). Elle permet aussi une certaine forme de généralisation: on peut supposer que les informations que l'on a à propos d'un mot se généraliseront aux mots à la distribution similaire. \n","\n","**Motivation**: On cherche à obtenir des représentations distribuées pour pouvoir, de manière **efficace**:\n","- Directement réaliser une analyse sémantique de surface.\n","- S'en servir comme source d'informations pour d'autres modèles et applications liées au language, notamment pour l'analyse de sentiments. \n","\n","\n","**Terminologie**: Attention à ne pas confondre l'idée de représentation *distribuée* et *distributionelle*. Le second indique en général (pour les mots) que la représentation a été obtenue strictement à partir de comptes de co-occurences, alors qu'on pourra utiliser des informations supplémentaires (labels de documents, tags de partie du discours, ...) pour construire des représentations distribuées. \n","Les modèles qui permettent de construire ces représentations denses, sous forme de vecteurs, sont souvent appellés *vector spaces models*. On appelle aussi régulièrement ces représentations des *word embeddings*, car les mots sont embarqués (*embedded*) dans un espace vectoriel. En Français, on rencontre souvent le terme *plongements de mots* ou *plongements lexicaux*."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WgsBru6eHA27"},"source":["## Obtenir une représentation: comptes d'occurences et de co-occurences\n","\n","Selon le type de corpus dont on dispose, on pourra obtenir différents types d'informations distributionelles. Si l'on a accès à une collection de documents, on pourra ainsi choisir de compter le nombre d'occurence de chaque mot dans chacun des documents, pour obtenir une matrice $mots \\times documents$: c'est sur ce principe qu'est construit **Tf-Idf** (vu au TP précédent). On va maintenant s'intéresser à un cas plus général: on dispose d'une grande quantité de données sous forme de texte, et on cherche à obtenir des représentations de mots sous forme de vecteurs de taille réduite, sans avoir besoin d'un découpage en documents ou catégories. \n","\n","Supposons qu'on dispose d'un corpus contenant $T$ mots différents. On va construire une matrice $\\mathbf{M}$ de taille $T \\times T$ qui contiendra le nombre de co-occurences entre les mots. Il y aura différents facteurs à considérer lors de la construction de cette matrice: \n","- Comment définir le 'contexte' d'un mot, qui permettra de dire que les termes qu'il contient co-occurent avec ce mot ? On pourra choisir d'utiliser différentes échelles: le document, la phrase, le groupe nominal, ou tout simplement une fenêtre de $k$ mots, selon les informations que l'on cherche à capturer.\n","*Encore une fois, si par exemple notre corpus est divisé en $D$ documents, on pourra même s'intéresser aux liens distributionnels entre mots et documents: chacun de ces $D$ documents agira comme un \"contexte\", et on construit une matrice d'occurences $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$.* \n","- Comment quantifier l'importance des comptes ? Par exemple, on pourra donner un poids décroissant à une co-occurence selon la distance entre les deux mots concernés ($\\frac{1}{d+1}$ pour une séparation par $d$ mots).\n","- Faut-il garder tous les mots qui apparaissent dans le corpus ? En général, non. On verra que pour les grands corpus, le nombre de mots différents $T$ est énorme. Deuxièmement, même si le nombre de mots est raisonnable, on ne possèdera que très peu d'information distributionelle sur les mots les plus rares, et la représentation obtenue sera à priori de mauvaise qualité. Il faudra se poser la question de comment filtrer ces mots, et de comment traiter les mots qu'on choisit de ne pas représenter.  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OVhaRLhTHA2-"},"source":["#### Procédure\n","\n","Pour construire la matrice, on va dans un premier temps recueillir la liste des mots différents (ou le *vocabulaire* $V$) qui apparaissent dans le corpus sous forme de dictionaire {mots -> index}\n","Puis, pour chaque terme $w$ du corpus,\n","- On récupère l'index $i$ correspondant à l'aide de $V$\n","- Pour chaque terme $w'$ du contexte de $w$, \n","  + On récupère l'index $j$ correspondant à l'aide de $V$\n","  + On incrémente $\\mathbf{M}_{i,j}$ par le poids correspondant, ou par $1$. \n","  \n","La procédure est très proche de celle qu'on a suivi au TP précédent, excepté qu'il faut maintenant compter les mots suivant leur apparition \n","  \n","#### Exemple\n","\n","On considère le corpus suivant: \n","\n","*I walked down down the boulevard. I walked down the avenue. I ran down the boulevard. I walk down the city. I walk down the the avenue.*\n","\n","On choisit de définir le contexte d'un mot comme la phrase à laquelle il appartient, et de ne pas utiliser de poids. \n","On obtient la matrice suivante: \n","\n","|     *         | I | the | down | walked | boulevard | avenue | walk | ran | city |\n","|---------------|---|-----|------|--------|-----------|--------|------|-----|------|\n","| I             | 0 |      6 |    6 |   2 |         2 |      2 |   2 |    1 |    1 |\n","| the           | 6 |      2 |    7 |   2 |         2 |      3 |   3 |    1 |    1 |\n","| down          | 6 |      7 |    2 |   3 |         3 |      2 |   2 |    1 |    1 |\n","| walked        | 2 |      2 |    3 |   0 |         1 |      1 |   0 |    0 |    0 |\n","| boulevard     | 2 |      2 |    3 |   1 |         0 |      0 |   0 |    1 |    0 |\n","| avenue        | 2 |      3 |    2 |   1 |         0 |      0 |   1 |    0 |    0 |\n","| ran           | 2 |      3 |    2 |   0 |         0 |      1 |   0 |    0 |    1 |\n","| walk          | 1 |      1 |    1 |   0 |         1 |      0 |   0 |    0 |    0 |\n","| city          | 1 |      1 |    1 |   0 |         0 |      0 |   1 |    0 |    1 |"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ezQ2nCREHA2_","colab":{}},"source":["def clean_and_tokenize(text):\n","    \"\"\"\n","    Cleaning a document with:\n","        - Lowercase        \n","        - Removing numbers with regular expressions\n","        - Removing punctuation with regular expressions\n","        - Removing other artifacts\n","    And separate the document into words by simply splitting at spaces\n","    Params:\n","        text (string): a sentence or a document\n","    Returns:\n","        tokens (list of strings): the list of tokens (word units) forming the document\n","    \"\"\"        \n","    # Lowercase\n","    text = text.lower()\n","    # Remove numbers\n","    text = re.sub(r\"[0-9]+\", \"\", text)\n","    # Remove punctuation\n","    REMOVE_PUNCT = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n","    text = REMOVE_PUNCT.sub(\"\", text)\n","    # Remove HTML artifacts specific to the corpus we're going to work with\n","    REPLACE_HTML = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n","    text = REPLACE_HTML.sub(\" \", text)\n","    \n","    tokens = text.split()        \n","    return tokens"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UpmeBiXiHA3C"},"source":["### Obtenir un Vocabulaire:\n","\n","Cette fois, on va implémenter séparément une fonction retournant le vocabulaire. Il faudra ici pouvoir contrôler sa taille, que ce soit en indiquant un nombre maximum de mots, ou un nombre minimum d'occurences pour qu'on prenne en compte les mots. On ajoute, à la fin, un mot \"inconnu\" qui remplacera tous les mots qui n'apparaissent pas dans notre vocabulaire 'limité'. "]},{"cell_type":"code","metadata":{"id":"k2s4Lxr_UHHC","colab_type":"code","colab":{}},"source":["def vocabulary(corpus, count_threshold=1, voc_threshold=0):\n","    \"\"\"    \n","    Function using word counts to build a vocabulary\n","    Params:\n","        corpus (list of list of strings): corpus of sentences\n","        count_threshold (int): number of occurences necessary for a word to be included in the vocabulary\n","        voc_threshold (int): maximum size of the vocabulary - 0 (default) indicates there is no max\n","    Returns:\n","        vocabulary (dictionary): keys: list of distinct words across the corpus\n","                                 values: indexes corresponding to each word sorted by frequency \n","        vocabulary_word_counts (dictionary): keys: list of distinct words across the corpus\n","                                             values: word counts in the corpus\n","    \"\"\"\n","    word_counts = {} #keys: words, values: counts\n","\n","    for doc in corpus:\n","        text=clean_and_tokenize(doc)\n","        for w in text:\n","            if w not in word_counts:\n","                word_counts[w]=1\n","            else :\n","                word_counts[w]+=1\n","    \n","    vocabulary_word_counts = {}\n","    vocabulary={}\n","    \n","    to_del=[w for w in word_counts if word_counts[w]<count_threshold]\n","    for k in to_del:\n","        word_counts.pop(k)\n","        \n","    sorted_vocab=sorted(word_counts.items(), key=lambda t:t[1])\n","    sorted_vocab.reverse()\n","    \n","    if (len(sorted_vocab)>voc_threshold-1 and voc_threshold!=0): #prendre en compte UNK ajoute a la fin\n","        sorted_vocab=sorted_vocab[:voc_threshold-1]\n","    \n","    sorted_words=[sorted_vocab[i][0] for i in range(len(sorted_vocab))]\n","    sorted_counts=[sorted_vocab[i][1] for i in range(len(sorted_vocab))]\n","    \n","    vocabulary = dict(zip(sorted_words,[i for i in range(len(sorted_words))]))\n","    vocabulary_word_counts = dict(zip(sorted_words,sorted_counts))\n","\n","    vocabulary['UNK']=len(vocabulary)\n","    vocabulary_word_counts['UNK']=0\n","    \n","    return vocabulary, vocabulary_word_counts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DIW8eXjoHA3F","outputId":"f94eabda-1c89-4bb4-921b-b3097c6c329f","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1583809816767,"user_tz":-60,"elapsed":4522,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["# Example for testing:\n","\n","corpus = ['I walked down down the boulevard',\n","          'I walked down the avenue',\n","          'I ran down the boulevard',\n","          'I walk down the city',\n","          'I walk down the the avenue']\n","\n","voc, counts = vocabulary(corpus, count_threshold = 3)\n","print(voc)\n","print(counts)\n","\n","# We expect something like this:\n","#  {'down': 0, 'the': 1, 'i': 2, 'UNK': 3}\n","#  {'down': 6, 'the': 6, 'i': 5, 'UNK': 0}\n","\n","voc, counts = vocabulary(corpus)\n","print(voc)\n","print(counts)\n","# We expect something like this:\n","#  {'down': 0, 'the': 1, 'i': 2, 'walked': 3, 'boulevard': 4, 'avenue': 5, 'walk': 6, 'ran': 7, 'city': 8, 'UNK': 9}\n","#  {'down': 6, 'the': 6, 'i': 5, 'walked': 2, 'boulevard': 2, 'avenue': 2, 'walk': 2, 'ran': 1, 'city': 1, 'UNK': 0}"],"execution_count":4,"outputs":[{"output_type":"stream","text":["{'the': 0, 'down': 1, 'i': 2, 'UNK': 3}\n","{'the': 6, 'down': 6, 'i': 5, 'UNK': 0}\n","{'the': 0, 'down': 1, 'i': 2, 'walk': 3, 'avenue': 4, 'boulevard': 5, 'walked': 6, 'city': 7, 'ran': 8, 'UNK': 9}\n","{'the': 6, 'down': 6, 'i': 5, 'walk': 2, 'avenue': 2, 'boulevard': 2, 'walked': 2, 'city': 1, 'ran': 1, 'UNK': 0}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F9BsqGxMHA3I"},"source":["### Obtenir les co-occurences:\n","\n","La fonction prend en entrée le corpus (une liste de strings, correspondant aux documents, ou phrases) et un vocabulaire, ainsi que la taille de la fenêtre de contexte. On pourra aussi implémenter la solution la plus simple - que le contexte d'un mot soit le reste du document duquel il provient. \n","Enfin, on pourra implémenter la possibilité de faire décroitre linéairement l'importance d'un mot du contexte à mesure qu'on s'éloigne du mot d'origine. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZJJSPPuzHA3J","colab":{}},"source":["def co_occurence_matrix(corpus, vocabulary, window=0, distance_weighting=False):\n","    \"\"\"\n","    Params:\n","        corpus (list of list of strings): corpus of sentences\n","        vocabulary (dictionary): words to use in the matrix\n","        window (int): size of the context window; when 0, the context is the whole sentence\n","        distance_weighting (bool): indicates if we use a weight depending on the distance between words for co-oc counts\n","    Returns:\n","        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n","    \"\"\" \n","    l = len(vocabulary)\n","    M = np.zeros((l,l))\n","    for sent in corpus:\n","        # Obtenir la phrase:\n","        sent = clean_and_tokenize(sent)\n","        # Obtenir les indexs de la phrase grace au vocabulaire:\n","        sent_idx = []\n","        for word in sent:\n","            if word in vocabulary:\n","                sent_idx.append(vocabulary[word])\n","            else:\n","                sent_idx.append(vocabulary['UNK'])\n","        \n","        \n","        # Parcourir les indexs de la phrase et ajouter 1 / dist(i,j) à M[i,j] si les mots d'index i et j apparaissent dans la même fenêtre. \n","        for i, idx_i in enumerate(sent_idx):\n","            # On vérifie que le mot est reconnu par le vocabulaire:\n","            if idx_i > -1:\n","                # Si on considère un contexte limité:\n","                if window > 0:\n","                    # On crée une liste qui contient les indexs de la fenêtre à gauche de l'index courant 'idx_i'\n","                    l_ctx_idx = [idx for idx in sent_idx[max(i-window,0):i]]\n","                    \n","                    # A compléter\n","                # Si on considère que le contexte est la phrase entière:\n","                else:\n","                    # La liste qui contient le contexte à gauche du mot est plus facile à créer:\n","                    l_ctx_idx = [idx for idx in sent_idx[:i]]\n","                    # A compléter\n","                # On parcourt cette liste et on update M[i,j]:  \n","                for j, idx_j in enumerate(l_ctx_idx):\n","                    # ... en s'assurant que le mot correspondant à 'idx_j' est reconnu par le vocabulaire\n","                    if idx_j > -1:\n","                        \n","                        # Calcul du poids:\n","                        if distance_weighting:\n","                            if window > 0:\n","                                if window<i: #la window ne \"rentre\" pas a gauche du mot i\n","                                    dist=i-j\n","                                \n","                                else:\n","                                    dist=window-j\n","                                weight=1.0/dist\n","                                #weight= 1.0- j* 1.0/window\n","                                \n","                            else:\n","                                #dist=len(l_ctx_idx)-j\n","                                #weight=1.0/dist\n","                                weight=1\n","                                # A compléter \n","                        else:\n","                            weight = 1.0\n","                        M[idx_i, idx_j] += weight * 1.0\n","                        M[idx_j, idx_i] += weight * 1.0\n","    return M  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gu91snY3HA3N","outputId":"2cff7242-009b-41ac-cec6-daf936d21fbf","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1583809816770,"user_tz":-60,"elapsed":4468,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["print(co_occurence_matrix(corpus, voc, 3, True))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[[1.         5.16666667 1.33333333 1.25       1.08333333 0.83333333\n","  0.75       0.5        0.5        0.        ]\n"," [5.16666667 2.         2.         1.         0.53333333 0.78333333\n","  1.5        0.33333333 0.5        0.        ]\n"," [1.33333333 2.         0.         0.66666667 0.         0.\n","  0.66666667 0.         0.33333333 0.        ]\n"," [1.25       1.         0.66666667 0.         0.         0.\n","  0.         0.25       0.         0.        ]\n"," [1.08333333 0.53333333 0.         0.         0.         0.\n","  0.25       0.         0.         0.        ]\n"," [0.83333333 0.78333333 0.         0.         0.         0.\n","  0.         0.         0.25       0.        ]\n"," [0.75       1.5        0.66666667 0.         0.25       0.\n","  0.         0.         0.         0.        ]\n"," [0.5        0.33333333 0.         0.25       0.         0.\n","  0.         0.         0.         0.        ]\n"," [0.5        0.5        0.33333333 0.         0.         0.25\n","  0.         0.         0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.         0.         0.         0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zUxb2ZhfHA3Q"},"source":["#### Application à un vrai jeu de données\n","\n","On va chercher à obtenir ces comptes pour les données **imdb**:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Nw4BDFG9NQJU","outputId":"d999bc8c-4afa-4a59-c44a-e4e4d9e7f628","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583809816771,"user_tz":-60,"elapsed":4443,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LygKzF4mHA3R","outputId":"feadf9fe-891f-4d65-9345-39761627e067","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1583809818573,"user_tz":-60,"elapsed":6211,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["from glob import glob\n","\n","filenames_neg = sorted(glob(os.path.join('/content/drive/My Drive/Colab Notebooks/IA312_NLP','data', 'imdb1', 'neg', '*.txt')))\n","filenames_pos = sorted(glob(os.path.join('/content/drive/My Drive/Colab Notebooks/IA312_NLP','data', 'imdb1', 'pos', '*.txt')))\n","\n","texts_neg = [open(f, encoding=\"utf8\").read() for f in filenames_neg]\n","texts_pos = [open(f, encoding=\"utf8\").read() for f in filenames_pos]\n","texts = texts_neg + texts_pos\n","\n","# The first half of the elements of the list are string of negative reviews, and the second half positive ones\n","# We create the labels, as an array of [1,len(texts)], filled with 1, and change the first half to 0\n","y = np.ones(len(texts), dtype=np.int)\n","y[:len(texts_neg)] = 0.\n","\n","print(\"%d documents\" % len(texts))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["2000 documents\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fcyVgMVjHA3W"},"source":["#### Etude rapide des données\n","\n","On voudrait ici, avant de procéder, avoir une idée de ce que contiennent ces critiques de films. On va donc obtenir le vocabulaire (entier) et représenter les fréquences des mots, dans l'ordre (attention, il faudra utiliser une échelle logarithmique): on devrait retrouver la loi de Zipf. Cela nous permettra d'avoir une idée de la taille du vocabulaire qu'on pourra choisir : il s'agit de réaliser un compromis entre les ressources nécessaires (taille des objets en mémoire) et quantité d'informations qu'on peut en tirer (les mots rares peuvent apporter beaucoup d'informations, mais il est difficile d'en apprendre de bonnes représentations,car ils sont rares !)  "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WAyipGzjHA3X","outputId":"9aa5ad51-6df2-4916-b681-9599f1d4a7cf","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"status":"ok","timestamp":1583809820022,"user_tz":-60,"elapsed":7624,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["# We would like to display the curve of word frequencies given their rank (index) in the vocabulary\n","vocab, word_counts = vocabulary(texts)\n","#\n","#  -- A compléter --\n","#\n","idxs=[idx for idx in range(1,len(vocab.values())+1)]\n","tot=np.sum(list(word_counts.values()))\n","\n","freq=[f for f in word_counts.values()]/tot\n","x=np.log2(idxs)\n","y_freq=np.log2(freq)\n","#print(x,y)\n","# We can for example use the function plt.scatter()\n","plt.figure(figsize=(20,5))\n","plt.title('Word counts versus rank')\n","#\n","#  -- A compléter --\n","#\n","plt.scatter(x,y_freq,marker='+')\n","#plt.yscale('log')\n","x_rep=5000\n","plt.axvline(np.log2(x_rep))\n","plt.show()\n","\n","# We would like to know how much of the data is represented by the 'k' most frequent words\n","print('Vocabulary size: %i' % len(vocab))\n","print('Part of the corpus by taking the \"x\" most frequent words ?')\n","#\n","#  -- A compléter --\n","#\n","\n","fr=[f/tot for f in word_counts.values()]\n","rep=np.sum(fr[0:x_rep])\n","print('%f of the corpus is represented by taking the %i most frequent words'%(rep,x_rep))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log2\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABIYAAAE/CAYAAAAzEsgaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dfbjsZ1kf+u9NIiAYxEg0hLCbiKIi\n8YUugsF3QYKKpHqKG60WWi+D1Fr1iJSY69htj0qqXliLnnanvqCiEg4a8RhpJNqC2E1gQ8NbguUl\nAUKiBHcxgTTBHe7zx8wksycza629Zs2atdZ8PteVizW/38zzPGvt35W9+OZ+7qe6OwAAAACsngcs\newEAAAAALIdgCAAAAGBFCYYAAAAAVpRgCAAAAGBFCYYAAAAAVpRgCAAAAGBFCYYAgKWqqkNV9fJl\nr4P5VdVzq+oNy14HALB5giEA4F5VdUlVvWbi2ntmXHv2zq5uZ1TV11fVzcteBwDAThAMAQDjXp/k\nyVV1SpJU1SOTfFqSr5i49vnD925aDfjdYxOW9bOqqlN3ek4AYLn8cgYAjHtzBkHQlw9ff02S/5rk\nryauva+7b0mSqnpyVb25qv5u+L9PHg1WVf+tqn6mqv4yyZ1JPq+qzq2q11XVHVX12iSPWG9BVXVR\nVV1XVbdX1fuq6unD62dV1R9V1bGqem9Vff/YZ15WVT899vqEKqCquqmqXlBVbx+u+4qqenBVPTTJ\na5KcVVUfH/5zVlWdX1VHh2v4m6p6yYy13lBVzxh7fWpV3VZVTxi+/sqq+u9V9bGqeltVff0GP6vn\nVtX7hz+rG6vqnwzfe8L2u6o6p6p6FOzM+tyU9R6qqldV1cur6vYkzx1+r0eGa7y1qn65qh449pmu\nqh8YVo19rKp+papqxvg/X1VvqKrPnHYfAFg+wRAAcK/u/mSSa5N87fDS1yb5iyRvmLj2+iSpqtOT\nXJXkPyT57CQvSXJVVX322LDfm+TiJKcl+UCS303ylgwCof87yXNmraeqzk/yW0l+PMnDh3PfNLz9\niiQ3JzkryT9O8rNV9Y0n8e1+Z5KnJzk3yZcmeW53fyLJNye5pbs/Y/jPLUl+KckvdffDkjwmyStn\njPl7Sb5r7PWFST7a3W+tqkdl8LP66SSnJ3lBkt+vqjPG3j/+s7otg5/rN3f3aUmenOS6jb6pYbh1\nMp+7KMmrMvj5/k6Se5L8aAZ/PhckeUqSfzHxmWckeWIGP7fvHH6f42t4QFX95+H9p3X33220bgBg\nOQRDAMCk1+W+EOhrMgiG/mLi2uuGX39rkvd092939/Hu/r0k707ybWPjvay739Xdx5M8MoNA4f/q\n7ru7+/VJ/r911vJ9SX69u1/b3Z/q7g9397ur6tFJvirJv+7uu7r7uiS/muSfnsT3+R+6+5buPjZc\nw5ev896/T/L5VfWI7v54d79xxvt+N8kzq+ohw9ffnUFYlCTfk+RPuvtPht/La5McTfItY58f/1kd\nT/KpJI+vqk/v7lu7+12b/N5O5nNHuvsPh2v63939lu5+4/DP86Ykh5N83cRnLuvuj3X3BzOoKBv/\n2X3a8Hs+Pcm3dfedm1wzALAEgiEAYNLrk3z1sBrojO5+T5L/nkHvodOTPD739Rc6K4MqoHEfSPKo\nsdcfGvv6rCT/a1iZM/7+WR6d5H1Trp+V5Fh337HOvBv567Gv70zyGeu89/uSPDbJu4fb5Z4x7U3d\n/d4kNyT5tmE49MwMwqIk+QdJnjXcfvWxqvpYkq/OICwb+dDYWJ9IcjDJDyS5taquqqov2uib2sLn\nxv98UlWPrao/rqq/Hm4v+9ncf7vfej+7z8+gCumnhhVoAMAuJhgCACYdSfKZSb4/yV8mSXffnuSW\n4bVbuvvG4XtvySDwGHcgyYfHXvfY17cm+azhdqfx98/yoQy2bk26JcnpVXXajHk/keQhY/fOXGeO\nSX2/C93v6e7vSvI5Sf5dkldNfA/jRtvJLkpy/TAsSgbfy29398PH/nlod182a+7uvrq7vymD8Ojd\nSf7zZr6/dT63me/3Pw4/8wXDrXM/kWRqD6EZbkjyz5K8pqq+8CQ+BwAsgWAIADhBd//vDLY4/Z8Z\nbCEbecPw2vhpZH+S5LFV9d3DRssHkzwuyR/PGPsDw7F/qqoeWFVfnRO3nU36tST/rKqeMuxb86iq\n+qLu/lAGVUwvHjaN/tIMqnpGDZmvS/ItVXV6VZ2Z5EdO4kfwN0k+e7xhclV9T1Wd0d2fSvKx4eVP\nzfj8K5I8Lcnzc1+1UIZr+7aqurCqThmu++ur6uxpg1TV59ag8fZDk9yd5ONjc16X5Gur6sBwnZds\n8nObcVqS25N8fFhp9PyT+GySZLil8CeSXFNV04I9AGCXEAwBANO8LoPqmDeMXfuL4bV7g6Hu/tsM\nGhH/WJK/TfLCJM/o7o+uM/Z3J3lSkmNJ/k0GzaWn6u43ZVB98otJ/m64rlGF0nclOSeD6qErk/yb\n7r5meO+3k7wtg0bVf5rkivW/3RPmfHcGVT/vH275OiuDJtXvqqqPZ9CI+tnDAG3a52/NoOrqyePz\nDsOsizIITG7LoILoxzP797EHZBDE3ZLBz+rrMgxphv2Jrkjy9gwaef/xZj63SS/I4M/ojgwqjTb9\nsxvX3b+Z5N8m+fOqOmcrYwAAi1fd96uWBgAAAGAFqBgCAAAAWFGCIQAAAIAVJRgCAAAAWFGCIQAA\nAIAVJRgCAAAAWFGnLnsB4x7xiEf0Oeecs+xlAAAA7Fvvv+0TSZLPO+OhS14JsFPe8pa3fLS7z5h2\nb1cFQ+ecc06OHj267GUAAADsWwcPH0mSXPG8C5a8EmCnVNUHZt2zlQwAAABgRQmGAAAAAFaUYAgA\nAABgRQmGAAAAAFaUYAgAAABgRS08GKqqH6uqrqpHLHouAAAAADZvocFQVT06ydOSfHCR8wAAAABw\n8hZdMfSLSV6YpBc8DwAAAAAnaWHBUFVdlOTD3f22Rc2xWx08fCQHDx9Z9jIAAAAA1nXqPB+uqmuS\nnDnl1qVJfiKDbWQbjXFxkouT5MCBA/MsBwAAAICTMFcw1N1PnXa9qs5Lcm6St1VVkpyd5K1VdX53\n//XEGJcnuTxJ1tbW9vSWs1GV0LU3Hjvh9RXPu2BpawIAAACYZa5gaJbufkeSzxm9rqqbkqx190cX\nMR8AAAAAJ28hwdCqGlUGqRQCAAAA9oIdCYa6+5ydmAcAAACAzVMxtAAqhQAAAIC9YGHH1QMAAACw\nuwmGAAAAAFaUYAgAAABgRQmGAAAAAFaUYAgAAABgRQmGAAAAAFaUYAgAAABgRQmGAAAAAFaUYIiF\nOXj4SA4ePrLsZQAAAAAzCIZYSUIrAAAASE5d9gLYf0aBy7U3Hjvh9RXPu2Bpa1qmVf/+AQAA2L0E\nQ6wUoRUAAADcRzDEthuFLKseuiw6hFr1ny8AAADzEwyxUoRWAAAAcB/BEAuz6qHLokIo2+EAAADY\nLoIhVpIQBQAAAARDsHDbHULZDgcAAMB2ecCyFwAsz8HDR+4NmAAAAFg9KoZgj1IpBAAAwLwEQ7CC\nNLAGAAAgsZUM2CLb0AAAAPa+hVYMVdUPJfnBJPckuaq7X7jI+YDN0cAaAACAZIHBUFV9Q5KLknxZ\nd99dVZ+zqLmAnWMbGgAAwP6xyIqh5ye5rLvvTpLu/sgC5wK2QJgDAACw2hYZDD02yddU1c8kuSvJ\nC7r7zQucD9gBtqEBAADsH3MFQ1V1TZIzp9y6dDj26Um+MskTk7yyqj6vu3tijIuTXJwkBw4cmGc5\nAAAAAJyEuYKh7n7qrHtV9fwkfzAMgt5UVZ9K8ogkt02McXmSy5NkbW2t7zcQsCupFAIAANj7Fnlc\n/R8m+YYkqarHJnlgko8ucD4AAAAATsIiewz9epJfr6p3JvlkkudMbiMDVoeeRAAAALvPwoKh7v5k\nku9Z1PgAAAAAzGeRFUMA91YKXXvjsRNeqxwCAABYvkX2GAIAAABgF1MxBCzUqDJIpRAAAMDuo2II\n2JUOHj5yb5gEAADAYqgYAnbEoiqFVCIBAABsnWAI2FUW0axaeAQAADCdYAjYk5x2BgAAMD/BELCr\nbGez6s2GR0IlAABgVQmGgD1pJ087W28OoRIAALCXCYaAXWk7gpaNwiPb0QAAgFUnGAL2tJ2oFJoW\nHG0UKqkyAgAA9gLBELDvzQpgdnI7GgAAwG4kGAKYYb3gaNa9eaqMAAAAdppgCFh5uyGYcWIaAACw\nDIIhgA2sF8pM3ttKldHJEhYBAADbRTAEsESztpeNbHbbmbAIAADYCsEQwAKcTJXRZl1/6+05ePiI\nHkUAAMC2EQwBLNFG28umNa8eNyssmhwfAABgGsEQwB4xLUSaFhbNMusENeERAACsLsEQwC4wK5zZ\nKLSZDHlGtrLdTHAEAACrRzAEsMecbFAzud3svENXJ0nuuOt4kvuHSrNMq1QSGgEAwN62sGCoqr48\nyX9K8uAkx5P8i+5+06LmA1hlJ9ubaNz1t96e5L6gaCvBkaAIAAD2pkVWDP1ckp/q7tdU1bcMX3/9\nAucDYIpZW8NmbT+bNC04Wi80EhIBAMDeschgqJM8bPj1Zya5ZYFzATDmZEKZeYKjydDo4OEjuf7W\n2/O4Rz5s3TFtRwMAgN1hkcHQjyS5uqp+IckDkjx5gXMBsIHJAGazgcxmTkMbhUbX33p77rjreK69\n8dj9QqL1CIkAAGA5qru3/uGqa5KcOeXWpUmekuR13f37VfWdSS7u7qdOGePiJBcnyYEDB/7hBz7w\ngS2vB4DFWa/aZzwEGoVEpz341HsriU578OC/Q4y/Hn39pHNPv1+ANDmH4AgAto+/V2H1VNVbuntt\n2r25KoamBT1jk/5Wkh8evvx/k/zqjDEuT3J5kqytrW09pQJgocZ/eZz2i+TjHvmwe0OcyZBoPeNV\nRqMAaRa/yAIAwPZa5FayW5J8XZL/luQbk7xngXMBsESTQc14SDR+f7IKaDJAmtbgevzrjYIjAADg\n5CzyN+zvT/JLVXVqkrsy3C4GwP52MtU8owDpvENXJ7kvGJpl8jQ0lUMAADCfhQVD3f2GJP9wUeMD\nsPut1/B6/OvJBtWT1UbTgqPrb709Bw8fmdoMW2AEAACboyYfgKWb3Go2ab3gaBoVRQAAsDmCIQB2\njVkVRZOnnx08fOTevkTjPYhGxnsRqSYCAIDZBEMA7BkbVQqNjPciGjW4HhEUAQDAfQRDAOwps044\nSwZ9h5L7gqHrb709d9x1/N7qouS+aqJRWDQraBIYAQCwCgRDAOwb45VBo0qh8VAouS80Gl2fDIpG\nVBYBALAKqruXvYZ7ra2t9dGjR5e9DAD2uM1UE21kVmA0IigCYK9ySAOsnqp6S3evTbunYgiAfWfa\nL7qTAc9GQdFGlUUqigAA2A8EQwDsa5OBzSjQmVUJNCswWi8oGp2UNmtOAADYrQRDAKyUWUHRyGYr\niyaDotH7HvfIh009DW30nsc98mGCIwAAdg3BEAArbVZIM6uyaDNB0WkPPjV33j27l9F4cCQkAgBg\nmQRDADDFZragbbaaaNy1Nx7LKXXiuEIiAACWRTAEAJswLSgahUSjYOf6W2/PnXcfzz0bHPg5ui8k\nAgBg2QRDALAF48HNrGqiyW1oR28aVBCNB0cbhUSjcQVFAAAsgmAIAOa0UUPrkYc86NR7w6LNhkQP\nedCp9445HhQl921lG782bT0AADCLYAgAttlGwczBw0c2HRLdcdfxqUHRLNffenvOO3R1EpVGAABs\nTDAEADtschvaRiHR+OtRUDRp/NooRLr+1ttPqF4SEgEAMEkwBABLtF5INGpmndw/KFrPeIh09KZj\n91YZnXfoalVEAACcQDAEALvEtIbWk0FRMr3HUJKpJ6Ld04OAKBlUEh296VgOHj4iHAIAIIlgCAB2\npc02tB7ZTJXReHPrx1xy1cx+RaqKAABWh2AIAPaAzQY146eXjcyqJLrz7uNTw6HxBtYjwiIAgP1J\nMAQA+8i07WjJYAvZetvMpjml7jsFbVpYNEl4BACw9wiGAGCfmgyJpoVD65kWHI2HRePuvHvQ6FqD\nawCAvWWuYKiqnpXkUJIvTnJ+dx8du3dJku9Lck+Sf9Xd6/9nRgBgYUZBzbStZiPTtpxN2qjKaHQS\n2qi6aNTvaO2c04VFAAC70LwVQ+9M8h1JDo9frKrHJXl2ki9JclaSa6rqsd19z5zzAQBzuOJ5F8wM\nh2ZVAp1MlVFyX3h0St13bXwrmooiAIDdY65gqLtvSJKqmrx1UZJXdPfdSW6sqvcmOT/J+keqAAAL\nt1EoMx4czTq5bGS94Gj8+niVkS1nAAC7x6J6DD0qyRvHXt88vHY/VXVxkouT5MCBAwtaDgCwWdPC\nmo2qjE6msmhUUXT0pmN5zCVX3TvO6JS0dxy6cOuLBwDgpGwYDFXVNUnOnHLr0u5+9bwL6O7Lk1ye\nJGtraydZrA4A7ISTqTIat14/omlB0p13Hz8hLEpsPQMAWKQNg6HufuoWxv1wkkePvT57eA0A2Icm\nT0AbhUTjfYbWqyiaFSCNTjsbhUWaWAMAbK9FbSX7oyS/W1UvyaD59BckedOC5gIAdpH1tqKdzJaz\naWHReEg0TmAEALA18x5X/+1JXprkjCRXVdV13X1hd7+rql6Z5Pokx5P8oBPJAGB1jUKbyS1no+Ps\nk/Uriqa9Z7wa6ehNx3LOi67KKSUkAgA4GdW9e9r6rK2t9dGjR5e9DABgBxw8fCRHbzqW5L7m0yOb\nrSqa5pQafH4UHN3TyWkP1tQaYOTg4cFh0UJ0WB1V9ZbuXpt2b1FbyQAA1jVry9nRm47llDoxLDqZ\noGj03vFwaNTUenRvvNpIhREAsMoEQwDArjEZ0Iz3Jppmo8BoPCSadf/aGwfb0EZsRwMAVolgCADY\ntSbDmfMOXX1CSLTZU88mrffeaWFRYjsaALA/CYYAgD1jVjBz3qGrc8ddx0/oKzTqNbRd7rjr+L1h\nkZAIANgvBEMAwJ43HtKMQqJkeiPq7SAkAgD2C8EQALCvTAtpxk9AS7Y3KBoPiUZj61EEAOwVgiEA\nYN+bFdKMAqPt3HI2rUfRk84VFAEAu5NgCABYWdPCmkWERdNOPnvfi791+yYAANgiwRAAwJhZlT3j\nvYvmdU/n3qBISAQALJNgCABgE2Y1uJ7XeEg0YusZALBTBEMAACdpPCR6zCVXbeu2s+T+W8+cfAYA\nLIpgCABgDpPbwLazmmhk8uQzQREAsF0EQwAA22gysFlEM+tRUGTLGQAwL8EQAMACTQtutquqaHzL\nmZAIANgKwRAAwA5bRCPrUUjklDMA4GQIhgAAlmhy69m8QdG0U84SFUUAwHSCIQCAXWRaj6Jrbzw2\n97iTJ52NaGQNAKtNMAQAsItNVvlsV1A04sQzAFhtgiEAgD1kFBRtV2+iSZNBUZLcdJmeRQCwXwmG\nAAD2oEU0sJ5l2hY0YREA7A+CIQCAPW48JHrMJVflnl78nJNhkebWALA3zRUMVdWzkhxK8sVJzu/u\no8Pr35TksiQPTPLJJD/e3X8+31IBANjItKPqF11RlJzY3Fo1EQDsHfNWDL0zyXckOTxx/aNJvq27\nb6mqxye5Osmj5pwLAIAtmNVMelGBkWbWALB3zBUMdfcNSVJVk9f/x9jLdyX59Kp6UHffPc98AABs\nn8nAZhFBkWbWALC77USPof8jyVuFQgAAu9u0yp5pjafnNWtM1UUAsPM2DIaq6pokZ065dWl3v3qD\nz35Jkn+X5GnrvOfiJBcnyYEDBzZaDgAAO2hadc8iwqLkxOqiU2p6vyQAYHttGAx191O3MnBVnZ3k\nyiT/tLvft874lye5PEnW1tZ24AwNAADmMRkWLeIktHv6xADK9jMAWIyFbCWrqocnuSrJi7r7Lxcx\nBwAAu8Oosufg4SO59sZjC5ljWpXSk849PVc874KFzAcAq2Le4+q/PclLk5yR5Kqquq67L0zyL5N8\nfpKfrKqfHL79ad39kblWCwDArjUZ0iyikmjctTcemxoYqS4CgM2b91SyKzPYLjZ5/aeT/PQ8YwMA\nsLdN6xG0qP5E682hsggAZtuJU8kAACDJ7GqeRVYXqSwCgNkEQwAALN14ddF5h67OHXcdX/ick2GR\nk9AAWEWCIQAAdpV3HLrw3q93KiRK7n8S2ojKIgD2M8EQAAC71nhING4nehXNmktQBMB+IhgCAGDP\nmRXOLKO59YjACIC9SDAEAMC+MRnOqCwCgPUJhgAA2Ld2U2WRoAiA3UgwBADAyllGZdH4HKc9+NSZ\n/ZMAYCcJhgAAWHnTqnkOHj6Sa288tpD57rjr+AlBkWoiAJZFMAQAAFNc8bwLTni9yKBINREAyyIY\nAgCATZgMika2OzCarCZKVBQBsDiCIQAAmMNkYHTeoatzx13Ht3WOaT2QhEUAbAfBEAAAbKNp28AW\n0dxaVREA20EwBAAACzYe2izqBLTxcU+p5H0vFhQBsDHBEAAA7KDJyp5FBEX3dJx6BsCmCIYAAGCJ\ndrqaaHJOAFabYAgAAHaJnagmmjauoAhgdQmGAABgl5oW2Bw8fCTX3nhsW+dx6hnA6hIMAQDAHnLF\n8y444fVjLrkq9/T2z6OqCGA1CIYAAGAPmzx9zPYzAE6GYAgAAPaRnWhmPW3sU+r+IRUAu99cwVBV\nPSvJoSRfnOT87j46cf9AkuuTHOruX5hnLgAA4OTsVDPrJLmnTxz/tAefmnccunBh8wGwPeatGHpn\nku9IcnjG/Zckec2ccwAAANtgJ4OiO+46bvsZwB4wVzDU3TckSVXd715V/aMkNyb5xDxzAAAAi7FT\np56NjAdFQiKA3WEhPYaq6jOS/Osk35TkBYuYAwAA2H6Tp57tVDPrEYERwM7aMBiqqmuSnDnl1qXd\n/eoZHzuU5Be7++PTqokmxr84ycVJcuDAgY2WAwAA7KBpQc1ONrUWFAEs1obBUHc/dQvjPinJP66q\nn0vy8CSfqqq7uvuXp4x/eZLLk2Rtba23MBcAALCDdrJXke1nAIu1kK1k3f01o6+r6lCSj08LhQAA\ngL1vp4IiIRHA9pv3uPpvT/LSJGckuaqqrutuZ1ICAMAK24mgyJYzgO0x76lkVya5coP3HJpnDgAA\nYG8TFAHsXgvZSgYAADDLTjS0FhQBbI5gCAAAWLpFVxXpTwQwnWAIAADYdcbDm0VXE03OB7BKBEMA\nAMCuNh7aHDx8JNfeeGzb5xgPi5507um54nkXbPscALuRYAgAANgzJgObRTSyvvbGY7aeAStDMAQA\nAOxZTjwDmI9gCAAA2Dd2OigSEgF7nWAIAADYtxbZxHramIIiYK8RDAEAACthWmjjxDNg1QmGAACA\nlbVTW89Oe/CpecehC7d9bIB5CYYAAACGFrX17I67jtt2BuxKgiEAAIApFl1NNBpPQAQsk2AIAABg\nE8YDnMdcclXu6e0ZV18iYJkEQwAAACfpfS/emWqiREgELJZgCAAAYE6LPPFMRRGwSIIhAACABRiF\nN4s66WxyHoCtEAwBAAAs0E41sZ42F8BGBEMAAAA7SCURsJsIhgAAAJZgkX2JJscSEgGzVPc2nbG4\nDdbW1vro0aPLXgYAAMDSLaKiaJLACFZDVb2lu9em3hMMAQAA7A2LDIuERLB/CYYAAAD2mUVXFAmK\nYP9YLxiaq8dQVT0ryaEkX5zk/O4+OnbvS5McTvKwJJ9K8sTuvmue+QAAABgYD252YtsZsD/N23z6\nnUm+I4MA6F5VdWqSlyf53u5+W1V9dpK/n3MuAAAAplhESKR5NayGuYKh7r4hSapq8tbTkry9u982\nfN/fzjMPAAAAm7PokGhyDmBvW9Rx9Y9N0lV1dZIzkryiu39uQXMBAAAwxbQA5+DhI7n2xmNzjTst\ncBIWwd60YTBUVdckOXPKrUu7+9XrjPvVSZ6Y5M4kfzZsdPRnU8a/OMnFSXLgwIHNrhsAAIAtetK5\np+eK512QJDnv0NW5467jc4+pqgj2pg2Doe5+6hbGvTnJ67v7o0lSVX+S5AlJ7hcMdfflSS5PBqeS\nbWEuAAAAtugdhy484bXtZ7BaHrCgca9Ocl5VPWTYiPrrkly/oLkAAAAA2IJ5j6v/9iQvzaCP0FVV\ndV13X9jd/6uqXpLkzUk6yZ90t/MTAQAAdrlFNK+eNZYqIli+eU8luzLJlTPuvTyDI+sBAADYgyaD\nm+1oXD3OdjNYvkWdSgYAAMA+M2pYPW47q4qAnScYAgAAYMsmq3zmCYpsN4OdJxgCAABg22xnUAQs\nnmAIAACAXWtWsKSSCLaHYAgAAICFmRbgqCKC3UMwBAAAwI7aju1m+hHB9njAshcAAAAAwHKoGAIA\nAGCptmu72XqfUU0E06kYAgAAAFhRKoYAAADYdWZV+Gy1cbVqIphOxRAAAADAilIxBAAAwJ4xrbrn\nMZdclXt662OOqolUDrGKBEMAAACQ2dvNBEbsZ4IhAAAA9rT3vXh2cLPVnkSwKvQYAgAAgHWc86Kr\nBEzsWyqGAAAA2LembQNzshncR8UQAAAAK+Wmy75VkANDgiEAAACYk+1m7FW2kgEAALCSZlUNzRPw\n2G7GXqNiCAAAAMbYasYqEQwBAADADrDdjN1orq1kVfWsJIeSfHGS87v76PD6pyX51SRPGM7xW939\n4vmWCgAAADtnvaohAQ/7xbw9ht6Z5DuSHJ64/qwkD+ru86rqIUmur6rf6+6b5pwPAAAAlm4UGm0l\nINrMZ2xlY6fMFQx19w1JUlX3u5XkoVV1apJPT/LJJLfPMxcAAAAA22tRp5K9KslFSW5N8pAkP9rd\nxxY0FwAAACzForabjT6rcohF2zAYqqprkpw55dal3f3qGR87P8k9Sc5K8llJ/qKqrunu908Z/+Ik\nFyfJgQMHNrtuAAAAAOa0YTDU3U/dwrjfneS/dPffJ/lIVf1lkrUk9wuGuvvyJJcnydraWm9hLgAA\nANh15ulDpFKInbKorWQfTPKNSX67qh6a5CuT/PsFzQUAAAD7yla3oQmUOFnzHlf/7UlemuSMJFdV\n1XXdfWGSX0nyG1X1riSV5De6++1zrxYAAAD2mJMJa+bpSwRbMe+pZFcmuXLK9Y9ncGQ9AAAAsIHt\nCoQ0reZkPWDZCwAAAABgOUEGsPMAAAhFSURBVBbVYwgAAADYpHkaVU8bBzZLxRAAAADAilIxBAAA\nALvEVit+RpVG29GrSNXRalExBAAAALCiVAwBAADAHrWI4+2dbLZaVAwBAAAArCgVQwAAALBHbddp\nZtPGZDWoGAIAAABYUSqGAAAAYI9T5cNWqRgCAAAAWFGCIQAAAIAVZSsZAAAAsK0NrNdj29vuomII\nAAAAYEWpGAIAAIAVtlOVQpPzqRzaHVQMAQAAAKwoFUMAAACwwkaVO3oMrSYVQwAAAAArSsUQAAAA\noJJnRakYAgAAAFhRgiEAAACAFSUYAgAAAFhRcwVDVfXzVfXuqnp7VV1ZVQ8fu3dJVb23qv6qqi6c\nf6kAAAAAbKd5K4Zem+Tx3f2lSf5nkkuSpKoel+TZSb4kydOT/D9VdcqccwEAAACwjeYKhrr7T7v7\n+PDlG5OcPfz6oiSv6O67u/vGJO9Ncv48cwEAAACwvbazx9A/T/Ka4dePSvKhsXs3D6/dT1VdXFVH\nq+robbfdto3LAQAAAGA9p270hqq6JsmZU25d2t2vHr7n0iTHk/zOyS6guy9PcnmSrK2t9cl+HgAA\nAICt2TAY6u6nrne/qp6b5BlJntLdo2Dnw0kePfa2s4fXAAAAgBV0zouuWvYSTtpNl33rspewcPOe\nSvb0JC9M8szuvnPs1h8leXZVPaiqzk3yBUneNM9cAAAAAGyvDSuGNvDLSR6U5LVVlSRv7O4f6O53\nVdUrk1yfwRazH+zue+acCwAAANhj9mKl0Mho7fu5cmiuYKi7P3+dez+T5GfmGR8AAACAxZm3YggA\nAABgplG1zV6sHNrPlUIj23lcPQAAAAB7iIohAAAAYOFWofpmL1IxBAAAALCiBEMAAAAAK0owBAAA\nALCiBEMAAAAAK0owBAAAALCiBEMAAAAAK0owBAAAALCiBEMAAAAAK0owBAAAALCiqruXvYZ7VdVt\nST6w7HVsk0ck+eiyF8Gu4XlgkmeCcZ4HxnkeGOd5YJzngXGeByat90z8g+4+Y9qNXRUM7SdVdbS7\n15a9DnYHzwOTPBOM8zwwzvPAOM8D4zwPjPM8MGmrz4StZAAAAAArSjAEAAAAsKIEQ4tz+bIXwK7i\neWCSZ4JxngfGeR4Y53lgnOeBcZ4HJm3pmdBjCAAAAGBFqRgCAAAAWFGCoQWoqqdX1V9V1Xur6kXL\nXg/LU1WPrqr/WlXXV9W7quqHl70mlq+qTqmq/1FVf7zstbBcVfXwqnpVVb27qm6oqguWvSaWp6p+\ndPh3xTur6veq6sHLXhM7q6p+vao+UlXvHLt2elW9tqreM/zfz1rmGtk5M56Hnx/+nfH2qrqyqh6+\nzDWyc6Y9D2P3fqyquqoesYy1sfNmPQ9V9UPDf0e8q6p+brPjCYa2WVWdkuRXknxzkscl+a6qetxy\nV8USHU/yY939uCRfmeQHPQ8k+eEkNyx7EewKv5Tkv3T3FyX5snguVlZVPSrJv0qy1t2PT3JKkmcv\nd1UswcuSPH3i2ouS/Fl3f0GSPxu+ZjW8LPd/Hl6b5PHd/aVJ/meSS3Z6USzNy3L/5yFV9egkT0vy\nwZ1eEEv1skw8D1X1DUkuSvJl3f0lSX5hs4MJhrbf+Une293v7+5PJnlFBn84rKDuvrW73zr8+o4M\n/k/fo5a7Kpapqs5O8q1JfnXZa2G5quozk3xtkl9Lku7+ZHd/bLmrYslOTfLpVXVqkockuWXJ62GH\ndffrkxybuHxRkt8cfv2bSf7Rji6KpZn2PHT3n3b38eHLNyY5e8cXxlLM+PdDkvxikhcm0Tx4hcx4\nHp6f5LLuvnv4no9sdjzB0PZ7VJIPjb2+OYIAklTVOUm+Ism1y10JS/bvM/jL+1PLXghLd26S25L8\nxnBr4a9W1UOXvSiWo7s/nMF/2ftgkluT/F13/+lyV8Uu8bndfevw679O8rnLXAy7yj9P8pplL4Ll\nqaqLkny4u9+27LWwKzw2yddU1bVV9bqqeuJmPygYgh1QVZ+R5PeT/Eh3377s9bAcVfWMJB/p7rcs\ney3sCqcmeUKS/9jdX5HkE7FFZGUN+8ZclEFgeFaSh1bV9yx3Vew2PThOWFUAqapLM2hZ8DvLXgvL\nUVUPSfITSX5y2Wth1zg1yekZtDD58SSvrKrazAcFQ9vvw0kePfb67OE1VlRVfVoGodDvdPcfLHs9\nLNVXJXlmVd2UwTbTb6yqly93SSzRzUlu7u5RFeGrMgiKWE1PTXJjd9/W3X+f5A+SPHnJa2J3+Juq\nemSSDP9301sD2J+q6rlJnpHknwzDQlbTYzL4jwlvG/5ueXaSt1bVmUtdFct0c5I/6IE3ZbBDYVMN\nyQVD2+/NSb6gqs6tqgdm0Djyj5a8JpZkmND+WpIbuvsly14Py9Xdl3T32d19Tgb/bvjz7lYRsKK6\n+6+TfKiqvnB46SlJrl/ikliuDyb5yqp6yPDvjqdEM3IG/ijJc4ZfPyfJq5e4Fpasqp6ewZb0Z3b3\nncteD8vT3e/o7s/p7nOGv1venOQJw98vWE1/mOQbkqSqHpvkgUk+upkPCoa22bAZ3L9McnUGv9C9\nsrvftdxVsURfleR7M6gMuW74z7cse1HArvFDSX6nqt6e5MuT/OyS18OSDCvHXpXkrUnekcHvaJcv\ndVHsuKr6vSRHknxhVd1cVd+X5LIk31RV78mgsuyyZa6RnTPjefjlJKclee3w98r/tNRFsmNmPA+s\nqBnPw68n+bzhEfavSPKczVYVlupDAAAAgNWkYggAAABgRQmGAAAAAFaUYAgAAABgRQmGAAAAAFaU\nYAgAAABgRQmGAAAAAFaUYAgAAABgRQmGAAAAAFbU/w+l/CSpfjjnUQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 1440x360 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Vocabulary size: 41682\n","Part of the corpus by taking the \"x\" most frequent words ?\n","0.887417 of the corpus is represented by taking the 5000 most frequent words\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oaMLz3OpHA3a"},"source":["Résultat de l'analyse: on peut se contenter d'un vocabulaire de 10000, voire 5000 mots - c'est important, car cela va déterminer la taille des objets que l'on va manipuler. On va maintenant recréer la matrice de co-occurence avec différents paramètres. Cela peut-être long: si cela pose problème, travaillez avec un vocabulaire plus réduit."]},{"cell_type":"markdown","metadata":{"id":"4aJiti9GXfJm","colab_type":"text"},"source":["Effectivement, prendre les 5000 mots les plus fréquents reviens à prendre la représentation de 88.7% du corpus, ce qui est la majorité de celui-ci et nous permet d'avoir des calculs moins couteux. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lXoNfqkpHA3b","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"3f97e1f6-3315-48ff-d28d-cac96bf47e7b","executionInfo":{"status":"ok","timestamp":1583809859340,"user_tz":-60,"elapsed":46915,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["'''\n","#Dummie data corpus\n","vocab_5k, word_counts_5k = vocabulary(corpus, 0, 5000)\n","M5dist = co_occurence_matrix(corpus, vocab_5k, window=5, distance_weighting=True)\n","M20 = co_occurence_matrix(corpus, vocab_5k, window=20, distance_weighting=False)\n","'''\n","vocab_5k, word_counts_5k = vocabulary(texts, 0, 5000)\n","M5dist = co_occurence_matrix(texts, vocab_5k, window=5, distance_weighting=True)\n","M20 = co_occurence_matrix(texts, vocab_5k, window=20, distance_weighting=False)\n","print(M5dist.shape)\n","print(M20.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["(5000, 5000)\n","(5000, 5000)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Tqai9LMsHA3f","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ed179983-2374-421d-80fa-69b5b3e2c311","executionInfo":{"status":"ok","timestamp":1583809859344,"user_tz":-60,"elapsed":46884,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["print(vocab_5k['cinema'])\n","print(M5dist[429])\n","print(M20[429])"],"execution_count":11,"outputs":[{"output_type":"stream","text":["833\n","[1.01882565 1.26467524 0.45274547 ... 0.         0.         1.88136857]\n","[ 730.  331.  304. ...    0.    0. 1178.]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hBFMabhmHA3j"},"source":["### Comparaison de vecteurs\n","\n","On peut se servir de ces vecteurs de très grande taille pour une analyse sémantique très basique: par exemple, en cherchant les plus proches voisins d'un mot. Cependant, il faudra faire attention aux distances qu'on utilise, liées à certaines métriques (Euclidiennes, Cosine) ou éventuellement d'autres liées à l'appartenance aux ensembles (Matching, Jaccard). La normalisation des vecteurs peut aussi jouer un rôle. Dans tous les cas, il faut bien faire attention à ne pas sur-interprêter ce type de résultats. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3e8wXu5vHA3k","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"259fc8d7-5d31-4bf9-9e19-c410e633ecd5","executionInfo":{"status":"ok","timestamp":1583809859679,"user_tz":-60,"elapsed":47194,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["def euclidean(u, v):\n","    return np.linalg.norm(u-v)\n","\n","def length_norm(u):\n","    return u / np.sqrt(u.dot(u))\n","\n","def cosine(u, v):\n","    return 1.0 - length_norm(u).dot(length_norm(v))\n","\n","from sklearn.neighbors import NearestNeighbors\n","\n","def print_neighbors(distance, voc, co_oc, mot, k=10):\n","    inv_voc = {id: w for w, id in voc.items()}\n","    neigh = NearestNeighbors(k, algorithm='brute', metric=distance)\n","    neigh.fit(co_oc) \n","    dist, ind = neigh.kneighbors([co_oc[voc[mot]]])\n","    print(\"Plus proches voisins de %s selon la distance '%s': \" % (mot, distance.__name__))\n","    print([[inv_voc[i] for i in s[1:]] for s in ind])\n","    \n","print(\"Avec un contexte large, sans prendre en compte la distance entre les mots:\")    \n","print_neighbors(euclidean, vocab_5k, M20, 'good')\n","print_neighbors(cosine, vocab_5k, M20, 'good')\n","print(\"\")\n","print(\"Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\")    \n","print_neighbors(euclidean, vocab_5k, M5dist, 'good')\n","print_neighbors(cosine, vocab_5k, M5dist, 'good') "],"execution_count":12,"outputs":[{"output_type":"stream","text":["Avec un contexte large, sans prendre en compte la distance entre les mots:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['story', 'can', 'much', 'also', 'will', 'time', 'character', 'we', 'films']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['bad', 'very', 'great', 'really', 'not', 'quite', 'it', 'fun', 'completely']]\n","\n","Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['bad', 'very', 'plot', 'comedy', 'just', 'little', 'there', 'man', 'had']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['very', 'bad', 'plot', 'great', 'not', 'its', 'funny', 'it', 'movie']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MvyDmkp-Sq_X","colab_type":"text"},"source":["Pour comparer les résultats des voisins de \"good\", je vais moi-même donner mon opinion pour savoir si les mots trouvés sont effectivement semblables à \"good\". Il est clair que c'est un avis subjectif, mais cela donne une idée de la qualité de ce clustering. On constate que :\n","\n","*   Avec un contexte large, égal à 20, les plus proches voisins de \"good\" pour la distance euclidienne ne sont pas similaires à good. Néanmoins, avec la distance cosinus, nous trouvons déjà au moins 4/10 mots semblables à \"good\".\n","*   Avec un contexte petit, de 5 mots, on peut associer au moins 2/10 à \"good\" des plus proches voisins avec la distance euclidienne et un peu plus (3/10) avec la distance cosine.\n","\n","Nous remarquons donc que globalement pour ce corpus les résultats sont meilleurs en prenant une fenêtre petite (de l'ordre de 5 mots) et surtout en utilisant la **distance cosine**. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4rEpKaj1HA3n"},"source":["### Méthodes de pondération des matrices\n","\n","**Motivation**: On ne se base pour l'instant que sur la fréquence (ou au mieux, une pondération de la fréquence) pour construire ces représentations. Comme on peut s'en douter, la fréquence seule n'est pas suffisante pour capturer des informations sémantiques intéressantes. On peut l'illustrer avec le phénomène des mots très fréquents qui apparaissent dans de nombreux contextes très différents, ou de mots qui apparaissent très souvent ensemble sans avoir nécessairement de lien sémantique."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Hgm5W2W7HA3o"},"source":["**Normalisation**: Très simple; il s'agit d'annuler l'influence de la magnitude des comptes sur la représentation.\n","\n","$$\\mathbf{m_{normalized}} = \\left[ \n","   \\frac{m_{1}}{\\sum_{i=1}^{n}m_{i}}, \n","   \\frac{m_{2}}{\\sum_{i=1}^{n}m_{i}}, \n","   \\ldots\n","   \\frac{m_{n}}{\\sum_{i=1}^{n}m_{i}}, \n","\\right]$$\n"," \n","**Pointwise Mutual Information**: Il s'agit d'évaluer à quel point la co-occurence des deux termes est *inattendue*. En effet, cette mesure correspond au ratio de la probabilité jointe des deux mots et du produit de leur probabilités individuelles:\n","$$\n","\\text{PMI}(x,y) = \\log \\left( \\frac{P(x,y)}{P(x)P(y)} \\right)\n","$$\n","La probabilité jointe des deux mots correspond au nombre de fois ou on les observe ensemble, divisé par le nombre total de co-occurences du corpus: \n","$$ P(\\mathbf{M},w_{1},w_{2}) = \\frac{M_{w_{1},w_{2}}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n","La probabilité individuelle d'un mot correspond simplement à sa fréquence, que l'on peut calculer en comptant toutes les co-occurences ou ce mot apparaît:\n","$$ P(\\mathbf{M},w) = \\frac{\\sum_{j=1}^{m} M_{w,j}}{\\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j}} $$\n","Ainsi,\n","$$ \n","\\text{PMI}(\\mathbf{M},w_{1},w_{2}) = \\log  \\frac{M_{w_{1},w_{2}} \\times \\left( \\sum_{i=1}^{n}\\sum_{j=1}^{n} M_{i,j} \\right)}{\\left( \\sum_{j=1}^{n} M_{w_{1},j} \\right) \\times \\left( \\sum_{i=1}^{n}M_{i,w_{2}} \\right)} \n","$$\n","On calcule ainsi le décalage entre l'observation que l'on a fait dans notre corpus et la fréquence d'apparition de ces termes si on les considère indépendant - c'est à dire qu'on suppose que leur co-occurence est une coïncidence.\n","\n","Le principal problème avec cette mesure est qu'elle n'est pas adaptée au cas où l'on observe aucune co-occurence. Puisque la PMI est censée renvoyer une quantité positive si l'on observe plus de co-occurences que prévu, et négative si l'on en observe moins, on ne peut pas choisir de remplacer $\\log(0)$ par $0$. Une solution couramment utilisée est d'utiliser la **Positive PMI**, qui fixe toutes les valeurs négatives à $0$.\n"," \n"," $$\\text{PPMI}(\\mathbf{M},w_{1},w_{2}) = \n"," \\begin{cases}\n"," \\text{PMI}(\\mathbf{M},w_{1},w_{2}) & \\textrm{if } \\text{PMI}(\\mathbf{M},w_{1},w_{2}) > 0 \\\\\n"," 0 & \\textrm{otherwise}\n"," \\end{cases}$$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1cGfBlMgHA3p","colab":{}},"source":["def pmi(co_oc, positive=True):\n","    sum_vec = co_oc.sum(axis=0)\n","    sum_tot = sum_vec.sum()\n","    with np.errstate(divide='ignore'):\n","        pmi = np.log((co_oc * sum_tot) / (np.outer(sum_vec, sum_vec)))                   \n","    pmi[np.isinf(pmi)] = 0.0  # log(0) = 0\n","    if positive:\n","        pmi[pmi < 0] = 0.0\n","    return pmi"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"67b2x9uZHA3s","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"07103e46-9028-489e-c84e-7c035c8f541c","executionInfo":{"status":"ok","timestamp":1583809862014,"user_tz":-60,"elapsed":49487,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["PPMI5 = pmi(M5dist)\n","PPMI20 = pmi(M20)\n","\n","print(\"Avec la PPMI:\")    \n","print_neighbors(euclidean, vocab_5k, PPMI5, 'good')\n","print_neighbors(cosine, vocab_5k, PPMI5, 'good')\n","print_neighbors(euclidean, vocab_5k, PPMI20, 'good')\n","print_neighbors(cosine, vocab_5k, PPMI20, 'good')"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Avec la PPMI:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['portions', 'text', 'UNK', 'the', 'redman', 'warned', 'clint', 'continued', 'potent']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['bad', 'but', 'for', 'its', 'comedy', 'and', 'it', 'funny', 'very']]\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['but', 'is', 'and', 'a', 'this', 'it', 'the', 'that', 'in']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['but', 'actors', 'pretty', 'acting', 'bad', 'better', 'very', 'performances', 'actor']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2-NDKr0xSq_o","colab_type":"text"},"source":["En utilisant la PPMI, qui nous donne une information sur l'occurence innattendue de deux mots, on voit que les résultats étaient mieux en utilisant simplement la matrice de co-occurence, tant que pour le grand que pour le petit contexte.\n","Ceci peut être dû au fait qu'en général les mots utilisés proche de \"good\" sont généralement positives, et donc pas inattendues."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RBRf8OlxHA3x"},"source":["**TF-IDF**: Comme on l'a déjà vu, il s'agit du produit de la fréquence du terme (TF) et de sa fréquence inverse dans les documents (IDF). \n","Cette méthode est habituellement utilisée pour extraire l'importance d'un terme $i$ dans un document $j$ relativement au reste du corpus, à partir d'une matrice $termes \\times documents$. Ainsi, pour une matrice $\\mathbf{X}$ de $n$ termes et $d$ documents: \n","\n"," $$\\text{TF}(X, i, j) = \\frac{X_{i,j}}{\\sum_{i=1}^{t} X_{i,j}} $$\n"," \n"," $$\\text{IDF}(X, i) = \\log\\left(\\frac{d}{|\\{j : X_{i,j} > 0\\}|}\\right)$$\n"," \n"," $$\\text{TF-IDF}(X, i, j) = \\text{TF}(X, i, j) \\cdot \\text{IDF}(X, i)$$\n","\n","\n","On peut l'adapter à notre cas en considérant que le contexte du deuxième mot est le document. Cependant, TF-IDF est généralement plus adaptée aux matrices peu denses, puisque cette mesure pénalisera les termes qui apparaissent dans une grande partie des documents. Ainsi, l'appliquer aux co-occurences des mots les plus fréquents n'est à priori pas optimal."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UpelrbfCHA3y","colab":{}},"source":["def tfidf(co_oc):\n","    \"\"\"\n","    Inverse document frequencies applied to our co_oc matrices\n","    \"\"\"\n","    # IDF\n","    d = float(co_oc.shape[1])\n","    in_doc = co_oc.astype(bool).sum(axis=1)\n","    with np.errstate(divide='ignore', invalid='ignore'):\n","        idfs = np.log(d / in_doc)\n","    \n","    idfs[np.isinf(idfs)] = 0.0  # log(0) = 0\n","    # TF\n","    sum_vec = co_oc.sum(axis=0)\n","    tfs = co_oc / sum_vec\n","\n","    return (tfs.T * idfs).T"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q3RdkTrlHA30","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"a17d30bc-5aef-4459-e278-c2a1dec8347e","executionInfo":{"status":"ok","timestamp":1583809862441,"user_tz":-60,"elapsed":49874,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["TFIDF5 = tfidf(M5dist)\n","\n","print(\"Avec TF-IDF:\")    \n","print_neighbors(euclidean, vocab_5k, TFIDF5, 'good')\n","print_neighbors(cosine, vocab_5k, TFIDF5, 'good')"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Avec TF-IDF:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['that', 'with', 'as', 'is', 'equivalent', 'in', 'masterful', 'but', 'for']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['a', 'comedy', 'is', 'will', 'of', 'in', 'suddenly', 'movie', 'moments']]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"czIHpvPzSq_1","colab_type":"text"},"source":["Là encore, nous avons des moins bons résultats que pour la simple utilisation de la matrice de co-occurences. Ceci peut être dû au fait que notre matrice est peu dense, vu que l'on compte les co-occurrence sur une fenêtre de petite dimension par rapport à la taille de notre matrice. Or, TF-IDF penalise les termes qui apparaissent souvent dans les documents. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zasdXQyaHA32"},"source":["### Matrice de co-occurences : Réduction de dimension\n","\n","#### Motivation\n","\n","Il s'agit non seulement de réduire la taille de données (ainsi, on traitera des vecteurs de dimension réduite, plutôt que de travailler avec des vecteurs de la taille du vocabulaire) mais aussi de mettre en évidence des relations de plus haut niveau entre les mots: en réduisant leurs représentations aux dimensions qui *les plus importantes* des données, on se retrouve à *généraliser* certaines propriétés entre les mots."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a5PqNVs4HA33"},"source":["#### Réduction de dimension via SVD \n","\n","Une matrice est une transformation linéaire: y appliquer une SVD, c'est décomposer notre transformation linéaire en un produit de transformations linéaires de différents types. Il va s'agir d'effectuer un changement de base, et de replacer nos données dans un espace ou chacune des coordonnées sont inchangées par la transformation effectuée. Ainsi, on décompose la matrice $\\mathbf{M}$ en trois matrices:\n","\n","$$ \\mathbf{M} = \\mathbf{U} \\mathbf{\\lambda} \\mathbf{V}^{\\text{T}} $$\n","\n","Les matrices $\\mathbf{U}$, $\\mathbf{\\lambda}$, et $\\mathbf{V}$ ont les propriétés suivantes:\n","- $\\mathbf{U}$ et $\\mathbf{V}$ sont des matrices orthogonales ($\\mathbf{U}^{\\text{T}} = \\mathbf{U}^{-1}$ et $\\mathbf{V}^{\\text{T}} = \\mathbf{V}^{-1}$). Elles contiennent les vecteurs propres à gauche et à droite de $\\mathbf{M}$.\n","- $\\mathbf{\\lambda}$ est une matrice diagonale: attention, elle n'est pas forcément carrée. Les coefficients de la diagonale sont les valeurs propres de $\\mathbf{M}$.\n","\n","Ainsi, les dmensions *les plus importantes* correspondent aux plus grandes valeurs propres. Réduire nos données à une dimension $k$ correspond à ne garder que les vecteurs correspondant aux $k$ premières valeurs propres - et cela revient à prendre les $k$ premiers vecteurs de la matrice $U$. \n","On utilise ici ```TruncatedSVD``` du package ```scikit-learn```:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6Gm1r-p6HA34","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"ef142e34-99ca-455f-8362-981ba01b3a0b","executionInfo":{"status":"ok","timestamp":1583809870410,"user_tz":-60,"elapsed":57808,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["from sklearn.decomposition import TruncatedSVD\n","svd = TruncatedSVD(n_components=300)\n","SVDEmbeddings = svd.fit_transform(M5dist)\n","print(SVDEmbeddings.shape)\n","SVDEmbeddings[vocab_5k['UNK']]\n","\n","print_neighbors(euclidean, vocab_5k, SVDEmbeddings, 'good')\n","print_neighbors(cosine, vocab_5k, SVDEmbeddings, 'good')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["(5000, 300)\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['bad', 'very', 'plot', 'comedy', 'just', 'little', 'there', 'man', 'had']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['very', 'bad', 'plot', 'funny', 'pace', 'simple', 'great', 'fine', 'shock']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vMT5p7R9Sq_-","colab_type":"text"},"source":["Cette fois-ci, nous obtenons des meilleurs et nous avons réduit la dimension de nos données (à 5000 x 300).\n","\n","\n","*   Pour la distance euclidienne, nous pouvons dire qu'il y a 2/10 mots proches de \"good\".\n","*   Pour la distance cosinus, il y a au moins 4/10 mots qui sont similaires à \"good\".\n","\n","Encore une fois, la distance cosine donne des meilleurs résulats.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xfkI-5H_HA37"},"source":["Note: Lorsque l'on applique cette méthode à la matrice des comptes $\\mathbf{M}$ de dimension $T \\times D$, où $\\mathbf{M}_{t,d}$ contient le nombre d'occurences du mot $t$ dans le document $d$, on obtient la méthode appellée **Latent Semantic Analysis**, pour la détection de composantes latentes (sémantiques) permettant de regrouper les documents.  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ECYPVrkOHA38"},"source":["#### Visualisation en deux dimensions\n","\n","On va maintenant utiliser **l'analyse en composantes principales** (PCA) pour visualiser nos données en 2 dimensions.  Cela revient à appliquer la SVD à la matrice de covariance des données, pour que les directions principales soient indépendantes les unes des autres et maximisent la variance des données.\n","On utilise la classe ```PCA``` du package ```scikit-learn```: "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QFhSldiTHA39","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"f5a4e77d-1d7c-467b-fda9-e1955e54c6e8","executionInfo":{"status":"ok","timestamp":1583809872449,"user_tz":-60,"elapsed":59820,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2, whiten=True)\n","Emb = pca.fit_transform(M5dist)\n","\n","words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n","         'dialog', 'role', 'actor', 'camera', 'scene',\n","         'film', 'movie', 'oscar', 'award']\n","ind_words = [vocab_5k[w] for w in words]\n","x_words = [Emb[ind,0] for ind in ind_words]\n","y_words = [Emb[ind,1] for ind in ind_words]\n","\n","fig, ax = plt.subplots()\n","ax.scatter(x_words, y_words)\n","\n","for i, w in enumerate(words):\n","    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXYAAAD4CAYAAAD4k815AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"mGwSPJCFncwD","colab_type":"text"},"source":["Lorsque nous visualisons les mots sur 2 composantes, nous remarquons deux groupes biens distincts (movie et film séparés du reste). On peut interpréter ceci en voyant que movie et film sont des mots plutot \"neutres\" et très utilisés dans le corpus."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HbYoc5DyHA4A","colab":{"base_uri":"https://localhost:8080/","height":265},"outputId":"09c9d77c-d2eb-4c44-ef41-fab6c11fe33b","executionInfo":{"status":"ok","timestamp":1583809874618,"user_tz":-60,"elapsed":61954,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["Norm5 = M5dist / np.linalg.norm(M5dist, ord=2, axis=1, keepdims=True)\n","pca = PCA(n_components=2, whiten=True)\n","Emb = pca.fit_transform(Norm5)\n","\n","words = ['bad', 'good', 'best', 'worst', 'poor', 'great',\n","         'dialog', 'role', 'actor', 'camera', 'scene',\n","         'film', 'movie', 'oscar', 'award']\n","ind_words = [vocab_5k[w] for w in words]\n","x_words = [Emb[ind,0] for ind in ind_words]\n","y_words = [Emb[ind,1] for ind in ind_words]\n","\n","fig, ax = plt.subplots()\n","ax.scatter(x_words, y_words)\n","\n","for i, w in enumerate(words):\n","    ax.annotate(w, (x_words[i], y_words[i]), (x_words[i] + 0.001, y_words[i] + 0.001))"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAD4CAYAAAD2FnFTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsZsD5f/h0n0AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"XrypA6jsoG47","colab_type":"text"},"source":["En normalisant nos occurences, nous avons une représentation plus claire sur les relations entre les mots. Nous remarquons que good et great sont très proches, mais que bad est aussi proche d'eux (ce qui semble logique car c'est un mot qui doit surement aparaître souvent près de good).\n","\n","Il y a aussi les mots relatifs au cinéma qui sont rassemblés, comme movie, scene, film, role, actor, award,...\n","\n","Il y a quelques mots \"mal placés\" comme best ou oscar, mais nous arrivons à voir des relations entre les mots."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JNraOfQ9HA4C"},"source":["## Obtenir une représentation: algorithmes couramment utilisés\n","\n","L'idée, ici, est de définir un ensemble de représentations ${w_{i}}_{i=1}^{V}$, de dimension prédéfinie $d$ (ici, on travaillera avec $d = 300$), pour tous les mots $i$ du vocabulaire $V$ - puis **d'entraîner** ces représentations pour qu'elles correspondent à ce que l'on souhaite. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2s45xGYLHA4D"},"source":["### Glove\n","\n","L'objectif défini par Glove ([Pennington et al. (2014)](http://www.aclweb.org/anthology/D/D14/D14-1162.pdf)) est d'apprendre des vecteurs $w_{i}$ et $w_{k}$ de façon à ce que leur produit scalaire correspondent au logarithme de leur **Pointwise Mutual Information**: \n","\n","\n","$$ w_{i}^\\top w_{k} = (PMI(w_{i}, w_{k}))$$\n","\n","\n","Dans l'article, l'obtention de cet objectif est minutieusement justifié par un raisonnement sur les opérations que l'on veut effectuer avec ces vecteurs et les propriétés qu'ils devraient avoir - notamment, une symétrie entre les lignes et les colonnes (voir l'article pour plus de détails).  \n","L'objectif final obtenu est le suivant, où $M$ est la matrice de co-occurences:\n","\n","\n","$$\\sum_{i, j=1}^{|V|} f\\left(M_{ij}\\right)\n","  \\left(w_i^\\top w_j + b_i + b_j - \\log M_{ij}\\right)^2$$\n","  \n"," \n","Ici, $f$ est une fonction de *mise à l'échelle* qui permet de diminuer l'importance des comptes de co-occurences les plus fréquents: \n","\n","\n","$$f(x) \n","\\begin{cases}\n","(x/x_{\\max})^{\\alpha} & \\textrm{if } x < x_{\\max} \\\\\n","1 & \\textrm{otherwise}\n","\\end{cases}$$\n","\n","\n","En général, on choisit $\\alpha=0.75$ et $x_{\\max} = 100$, même si ces paramètres peuvent nécessiter un changement selon les données."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5_nj2C7GHA4E"},"source":["Le code suivant utilise l'API de gensim pour récupérer des représentations pré-entrainées (Il est normal que le chargement soit long)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"41JxHBDXHA4F","colab":{"base_uri":"https://localhost:8080/","height":71},"outputId":"80b94f6c-7a5f-480c-d868-5c77245c813c","executionInfo":{"status":"ok","timestamp":1583810007557,"user_tz":-60,"elapsed":194868,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["import gensim.downloader as api\n","loaded_glove_model = api.load(\"glove-wiki-gigaword-300\")"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jrmBCJ80HA4H"},"source":["On peut extraire la matrice des embeddings ainsi, et vérifier sa taille:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5OunHBdbHA4I","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"76b6412e-33c7-4963-b7e9-36e4385b2ec6","executionInfo":{"status":"ok","timestamp":1583810007559,"user_tz":-60,"elapsed":194839,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["loaded_glove_embeddings = loaded_glove_model.vectors\n","print(loaded_glove_embeddings.shape)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["(400000, 300)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3fR32_dOHA4M"},"source":["On voit donc qu'il y a $400.000$ mots représentés, et que les embeddings sont de dimension $300$. On définit une fonction qui nous renvoie, à partir du modèle chargé, le vocabulaire et la matrice des embeddings suivant les structures que l'on a utilisé auparavant. On ajoute, ici encore, un mot inconnu ```'UNK'``` au cas où se trouve dans nos données des mots qui ne font pas parti des $400.000$ mots représentés ici. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8sbsrH2pHA4N","colab":{}},"source":["def get_glove_voc_and_embeddings(glove_model):\n","    voc = {word : index for word, index in enumerate(glove_model.index2word)}\n","    voc['UNK'] = len(voc)\n","    embeddings = glove_model.vectors\n","    return voc, embeddings"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bXqu8MKjHA4R","colab":{}},"source":["loaded_glove_voc, loaded_glove_embeddings = get_glove_voc_and_embeddings(loaded_glove_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qAAtYtk1HA4W"},"source":["Afin de comparer 'à jeu égal' les représentations chargées ici et celles que l'on a produite, il faudrait utiliser le même vocabulaire. Dans ce but, je réutilise le code qui suit pour créer un vocabulaire de $5000$ mots à partir des données exactement comme hier, et j'ajoute à la fin une fonction qui renvoie la matrices des représentations chargées avec Glove pour ces $5000$ mots seulement, dans le bon ordre. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p9h7o6LzHA4X","colab":{}},"source":["def get_glove_adapted_embeddings(glove_model, input_voc):\n","    keys = {i: glove_model.vocab.get(w, None) for w, i in input_voc.items()}\n","    index_dict = {i: key.index for i, key in keys.items() if key is not None}\n","    embeddings = np.zeros((len(input_voc),glove_model.vectors.shape[1]))\n","    for i, ind in index_dict.items():\n","        embeddings[i] = glove_model.vectors[ind]\n","    return embeddings"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bK9dfBkkHA4a","colab":{}},"source":["GloveEmbeddings = get_glove_adapted_embeddings(loaded_glove_model, vocab_5k)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-x70zV0uHA4d"},"source":["Cette fonction prend donc en entrée le modèle chargé à l'aide de l'API Gensim, ainsi qu'un vocabulaire que nous avons créé nous même, et renvoie la matrice d'embeddings tiré du modèle chargé, pour les mots notre vocabulaire et dans le bon ordre.\n","Remarque: les mots inconnus sont représentés par le vecteur nul:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dQoZPEJyHA4d","colab":{"base_uri":"https://localhost:8080/","height":340},"outputId":"c908a45f-210d-448b-ee09-e6fd1a989186","executionInfo":{"status":"ok","timestamp":1583810007569,"user_tz":-60,"elapsed":194710,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["print(GloveEmbeddings.shape)\n","GloveEmbeddings[vocab_5k['UNK']]"],"execution_count":26,"outputs":[{"output_type":"stream","text":["(5000, 300)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mm6nsrAOHA4j","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"c5746d9a-64ca-49a4-af60-a2d1cff17422","executionInfo":{"status":"ok","timestamp":1583810007570,"user_tz":-60,"elapsed":194685,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["print_neighbors(euclidean, vocab_5k, GloveEmbeddings, 'good')\n","print_neighbors(cosine, vocab_5k, GloveEmbeddings, 'good')"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Plus proches voisins de good selon la distance 'euclidean': \n","[['better', 'well', 'always', 'really', 'sure', 'way', 'so', 'but', 'excellent']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['better', 'really', 'always', 'you', 'well', 'excellent', 'very', 'things', 'think']]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n","  \"\"\"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"HoF-qJ40p5JB","colab_type":"text"},"source":["Nous remarquons ici que les résultats sont beaucoup plus satisfaisants qu'avant. Dans les deux cas des distances, nous avons au moins 5/10 similaires à \"good\".\n","Contrairement à avant, il n'y a pas de grande distance au niveau des résultats entre la distance euclidienne et la distance cosinus.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Yb49MNGAHA4m"},"source":["### Word2Vec\n","\n","\n","#### Le modèle skip-gram\n","\n","Le modèle skip-gram de base estime les probabilités d'une paire de mots $(i, j)$ d'apparaître ensemble:\n","\n","\n","$$P(j \\mid i) = \\frac{\\exp(w_{i} c_{j})}{\\sum_{j'\\in V}\\exp(w_{i} c_{j'})}$$\n","\n","\n","où $w_{i}$ est le vecteur ligne (du mot) $i$ et $c_{j}$ est le vecteur colonne (d'un mot du contexte) $j$. L'objectif est de minimiser la quantité suivante: \n","\n","\n","$$ -\\sum_{i=1}^{m} \\sum_{k=1}^{|V|} \\textbf{1}\\{o_{i}=k\\} \\log \\frac{\\exp(w_{i} c_{k})}{\\sum_{j=1}^{|V|} \\exp(w_{i} c_{j})}$$\n","\n","\n","ou $V$ est le vocabulaire.\n","Les entrées $w_{i}$ sont les représentations des mots, que l'on met à jour pendant l'entraînement, et la sortie est un vecteur *one-hot* $o$, qui ne contient qu'un seul $1$ et des $0$. Par exemple, si `good` est le  47ème mot du vocabulaire, la sortie $o$ pour un exemple ou `good` est le mot à prédire consistera en des $0$s partout sauf un $1$ en 47ème position du vecteur. `good` sera le mot à prédire lorsque l'entrée $w$ sera un mot de son contexte\n","On obient donc cette sortie avec softmax standard - on ajoute un terme de biais $b$.\n","\n","\n","$$ o = \\textbf{softmax}(w_{i}C + b)$$\n","\n","\n","Si l'on utilise l'ensemble des représentations pour tout le vocabulaire (la matrice $W$) comme entrée, on obtient \n","\n","\n","$$ O = \\textbf{softmax}(WC + b)$$\n","\n","\n","et on revient ainsi à l'idée centrale de toutes nos méthodes: on cherche à obtenir des représentations de mots à partir de comptes de co-occurences. Ici, on entraîne les paramètres contenus dans $W$ et $C$, deux matrices représentants les mots en dimension réduite (300) de façon à ce que leur produit scalaire soit le plus proche possible des co-occurences observées dans les données, à l'aide d'un objectif de maximum de vraisemblance.\n","\n","#### Le skip gram avec negative sampling\n","\n","L'entraînement du modèle skip-gram implique de calculer une somme sur l'ensemble du vocabulaire, à cause du **softmax**. Dès que la taille du vocabulaire augmente, cela devient infaisalbe. Afin de rendre les calculs plus rapides, on change l'objectif et on utilise la méthode du *negative sampling* (ou, celle, très proche, du *noise contrastive estimation*).\n","\n","\n","Si on note $\\mathcal{D}$ l'ensemble des données et que l'on not $\\mathcal{D}'$ un ensemble de paires de mots qui ne sont **pas** dans les données (et qu'en pratique, l'on tire aléatoirement, l'objectif est:\n","\n","\n","$$\\sum_{i, j \\in \\mathcal{D}}-\\log\\sigma(w_{i}c_{j}) + \\sum_{i, j \\in \\mathcal{D}'}\\log\\sigma(w_{i}c_{j})$$\n","\n","\n","ou $\\sigma$ est la fonction d'activation sigmoide $\\frac{1}{1 + \\exp(-x)}$.\n","Une pratique commune est de générer les paires de $\\mathcal{D}'$ de manière proportionelle aux fréquences des mots dans les données d'entraînement (ce qu'on appelle la distribution unigramme):\n","\n","\n","$$P(w) = \\frac{\\textbf{T}(w)^{0.75}}{\\sum_{w'\\in V} \\textbf{T}(w')}$$\n","\n","\n","Bien que différente, cette nouvelle fonction objectif est une approximation suffisante de la précédente, et est basée sur le même principe. De nombreuses recherches ont été effectuées sur cet objectif: par exemple, [Levy and Golberg 2014](http://papers.nips.cc/paper/5477-neural-word-embedding-as-implicit-matrix-factorization) démontre que l'objectif calcule la matrice de PMI décalé d'une valeur constante. On peut aussi voir [Cotterell et al. 2017](https://aclanthology.coli.uni-saarland.de/papers/E17-2028/e17-2028) pour une interprétation de l'algorithme comme une variante de la PCA."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"t1IYxO2aHA4m"},"source":["On va utiliser la bibliothèque ```gensim``` pour son implémentation de word2vec en python. On va devoir en faire une utilisation un peu spécifique, puisqu'on veut conserver le même vocabulaire qu'auparavant: on va d'abord créer la classe, puis récupérer le vocabulaire qu'on a utilisé plus haut. \n","Pour ne pas à avoir à mettre toutes les données en mémoire d'un coup, on définit un générateur, qui prendra toutes les données en entrée et les pré-traitera et renverra à la classe ```Word2Vec``` phrase par phrase. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B19UcBfdHA4n","colab":{}},"source":["from gensim.models import Word2Vec\n","\n","model = Word2Vec(size=300,\n","                 window=5,\n","                 null_word=1,\n","                 iter=30)\n","model.build_vocab_from_freq(word_counts_5k)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"C_dmI_FEHA4q","colab":{}},"source":["def preprocess_generator(large_corpus):\n","    for line in large_corpus:\n","        yield clean_and_tokenize(line)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ajdBnqRFHA4s","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0e79f6d1-03eb-4ee6-c704-da65519f5d75","executionInfo":{"status":"ok","timestamp":1583810011850,"user_tz":-60,"elapsed":198887,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["model.train(preprocess_generator(texts[:]), total_examples=10, epochs=30, report_delay=1)"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(850729, 1305272)"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8DinyWpDHA4v","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"d5443962-97f1-43c8-d31e-91c4e1494530","executionInfo":{"status":"ok","timestamp":1583810011852,"user_tz":-60,"elapsed":198852,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["W2VEmbeddings = model.wv.vectors\n","print(W2VEmbeddings.shape)\n","W2VEmbeddings[vocab_5k['UNK']]"],"execution_count":31,"outputs":[{"output_type":"stream","text":["(5000, 300)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([-1.60300045e-03,  8.69550451e-04, -1.32573373e-03,  4.53727262e-04,\n","       -1.23742793e-03, -9.20572842e-04,  1.57892960e-03, -1.03891897e-03,\n","        6.56333752e-04, -3.84118001e-04,  7.04426377e-04, -3.49659793e-04,\n","        5.94485435e-04, -1.20179553e-03, -1.13312795e-03,  1.21739379e-03,\n","       -2.31930280e-05,  4.30091721e-04, -1.23466295e-03,  8.82986424e-05,\n","        3.20205028e-04,  6.95804483e-04, -4.14463488e-04, -7.73179054e-05,\n","        2.28320729e-04,  1.72373999e-04, -1.42576278e-03,  3.85104417e-04,\n","       -4.16688883e-04, -5.81444707e-04, -1.50297140e-03, -1.58043148e-03,\n","       -1.21155847e-03,  3.77119257e-04, -9.30641370e-04, -1.35743187e-03,\n","        1.23046193e-04, -3.07486596e-04, -1.09631056e-03, -1.37596752e-03,\n","       -1.55815156e-03,  1.30715407e-03,  3.31498377e-05, -9.81632969e-04,\n","        1.55736855e-03, -1.02523167e-03,  1.21634628e-03,  1.30835280e-03,\n","        7.24194397e-05, -3.73915740e-04,  3.70833353e-04, -1.65978586e-03,\n","        1.08086539e-03, -1.04311691e-03, -7.53310509e-04, -9.69105517e-04,\n","       -9.24509601e-04, -1.24220480e-03,  1.30537990e-03, -1.50441821e-03,\n","        5.49881719e-04, -1.21912896e-03, -8.70634394e-04, -1.35093171e-03,\n","       -1.40498113e-03,  5.37553162e-04, -5.68281102e-04, -1.52536275e-04,\n","       -1.13129488e-03, -4.57548944e-04, -1.19380991e-03,  3.17229045e-04,\n","       -9.32264200e-04,  8.19645997e-04,  1.53806177e-03,  3.45127664e-05,\n","        8.96111596e-04,  9.98195261e-04,  1.58013159e-03,  1.56580913e-03,\n","       -3.55391676e-04,  1.52066606e-03, -1.42805651e-03, -7.98449852e-04,\n","       -1.04407826e-03,  8.44056951e-04, -4.80861112e-04, -1.36940600e-03,\n","        1.18467631e-03,  1.32073867e-04, -1.48643157e-03,  1.64515397e-03,\n","       -1.65303602e-04, -1.81763739e-04,  1.34740176e-03,  1.15471904e-03,\n","       -1.48462458e-03, -1.21764909e-03,  8.96514626e-04, -5.11442195e-04,\n","       -6.37317891e-04,  1.32121716e-03, -1.21507284e-04, -1.32493966e-03,\n","        1.02117076e-03, -3.78756056e-04,  1.10936537e-03,  3.96377261e-04,\n","        1.08953344e-03,  1.40177330e-03, -1.48854940e-03,  8.47882067e-04,\n","       -7.90250080e-04, -1.14359202e-04, -1.27950264e-03, -3.91180161e-04,\n","       -1.28795579e-03,  1.38939905e-03, -1.15068513e-03,  4.57560818e-05,\n","       -1.65213551e-03,  4.24885890e-04, -9.54833085e-05,  1.09058950e-04,\n","        1.05393468e-03,  5.05832490e-04, -6.71315274e-06, -1.41274428e-03,\n","        1.64782512e-03, -1.35288946e-03,  6.33393065e-04,  1.65993616e-03,\n","        8.10530910e-04, -1.07165158e-03,  7.55311688e-04,  1.07108871e-03,\n","        1.25802949e-03, -1.41018059e-03, -1.19927363e-03,  2.13586638e-04,\n","       -4.20820579e-04,  1.21124751e-04,  1.57989888e-03,  9.14678967e-04,\n","        1.00372802e-03,  5.89025090e-04, -3.12419666e-04,  2.89036077e-04,\n","       -2.56406056e-04,  1.50628609e-03, -1.06274756e-03,  1.18164835e-03,\n","        1.86298465e-04,  2.78579188e-04, -1.36244670e-03, -1.32058782e-03,\n","        1.37693004e-03,  3.31501564e-04, -1.63468928e-03, -1.58750615e-03,\n","       -2.54285318e-04,  3.37195743e-05, -4.53566900e-04, -1.33228209e-03,\n","       -9.99492477e-04,  8.86684109e-04, -1.27284508e-03, -1.72629516e-04,\n","       -4.18451731e-04,  5.73985395e-04, -9.99839976e-04, -1.21132797e-03,\n","        1.62749970e-03,  6.19090337e-04, -2.27392098e-04, -1.64541579e-03,\n","       -7.16091192e-04,  5.68494033e-05,  1.44850998e-03,  5.98087732e-04,\n","        1.50599389e-03,  1.09272939e-03, -1.50644788e-04,  1.32347469e-03,\n","       -9.12839663e-04,  5.72329620e-04,  1.58571068e-03, -1.59664417e-03,\n","       -9.17600002e-04, -1.07882870e-03,  7.16240669e-04,  1.55642745e-03,\n","       -5.96002850e-04, -2.88555108e-04, -1.54644519e-03, -1.52648496e-03,\n","       -1.38894212e-03, -1.79447336e-04,  4.48618433e-04, -9.82025056e-04,\n","        1.15682685e-03, -1.22927348e-04,  1.28844567e-03, -1.56196102e-03,\n","       -2.23419465e-05, -5.22646529e-04, -2.54284590e-04,  8.91705044e-04,\n","       -5.23315335e-04,  9.47323788e-05, -1.48184353e-03, -1.57706230e-03,\n","       -1.16508898e-04,  5.33270475e-04,  7.90155900e-04,  2.11195293e-04,\n","       -7.32864719e-04,  8.42924230e-04,  5.99659630e-04,  2.96750019e-04,\n","        7.95267406e-04,  1.42887607e-03,  6.66351290e-04,  1.20330439e-03,\n","       -2.08553829e-05,  8.00521870e-04,  2.72520032e-04,  1.20389427e-03,\n","        5.05550765e-04,  1.05338018e-04,  1.07668655e-03, -1.49638974e-03,\n","        1.35769625e-03, -4.53648681e-04,  1.10469968e-03, -8.57874693e-05,\n","        7.12352630e-04, -2.34182880e-04,  8.93457909e-04,  7.38879899e-04,\n","       -1.50397397e-03, -1.09613710e-03,  9.60977166e-04, -8.57453735e-04,\n","       -8.88672075e-04,  1.04086869e-03, -9.18421894e-04,  1.10872404e-03,\n","        2.41454909e-04,  1.59974897e-03, -8.76301783e-04, -2.91519944e-04,\n","       -4.76928952e-04,  1.17295259e-03,  1.18436897e-03,  1.26781466e-03,\n","        5.77020051e-04,  1.06947997e-03,  1.63113885e-03, -1.32512918e-03,\n","       -2.21290829e-04,  6.60653866e-04,  1.28905079e-03,  7.11682136e-04,\n","        1.31861656e-03, -1.17584095e-04, -5.14149491e-04, -1.16001419e-03,\n","       -1.31506566e-03, -6.47739798e-04, -7.79951341e-04,  2.33464234e-04,\n","        1.28801644e-03, -3.13558092e-04,  1.73205190e-04,  5.28971083e-04,\n","        3.49219248e-04,  1.31025235e-03, -7.01099925e-04, -1.10302877e-03,\n","       -3.20206513e-04,  1.38489448e-03, -1.65013352e-03, -1.52126583e-03,\n","        9.79446806e-04,  6.72510534e-04, -5.05804201e-04, -9.75870644e-04,\n","        1.45295437e-03,  1.07861857e-03,  1.95209112e-04,  8.45393864e-04,\n","       -1.17646786e-03,  2.74310412e-04,  3.60517239e-04, -4.52600834e-05,\n","       -3.50001385e-04, -1.58777996e-03,  2.96214770e-04, -1.44626320e-04],\n","      dtype=float32)"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wtcXAzhPHA4y","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"86ab28e9-ca41-486e-c488-782126e79136","executionInfo":{"status":"ok","timestamp":1583810011854,"user_tz":-60,"elapsed":198826,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["print_neighbors(euclidean, vocab_5k, W2VEmbeddings, 'good')\n","print_neighbors(cosine, vocab_5k, W2VEmbeddings, 'good')"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Plus proches voisins de good selon la distance 'euclidean': \n","[['first', 'director', 'would', 'any', 'also', 'get', 'much', 'films', 'will']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['who', 'that', 'and', 'an', 'with', 'or', 'a', 'to', 'of']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sHVnjRALrvXL","colab_type":"text"},"source":["Nous remarquons que ici nous n'obtenons pas des résultats satisfaisants, les mots obtenus semblent appartenir à des contextes beaucoup plus généraux que l'opinion sur les films, ou à simplement être des mots très communs comme 'a', 'the', 'and',..."]},{"cell_type":"markdown","metadata":{"id":"B699xM06D6Gb","colab_type":"text"},"source":["Nous allons essayer de jouer sur les paramètres pour voir si nous obtenons des meilleurs résultats"]},{"cell_type":"code","metadata":{"id":"QfUGsP41EBYq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":612},"outputId":"dc9cae49-a66a-4379-a231-141103b7b051","executionInfo":{"status":"ok","timestamp":1583810041940,"user_tz":-60,"elapsed":228884,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["windows=[1,2,3,5,10,15,20]\n","for w in windows:\n","  model = Word2Vec(size=300,\n","                 window=w,\n","                 null_word=1,\n","                 iter=30)\n","  model.build_vocab_from_freq(word_counts_5k)\n","  model.train(preprocess_generator(texts[:]), total_examples=10, epochs=30, report_delay=1)\n","  W2VEmbeddings = model.wv.vectors\n","  print('----------WINDOW %i ---------' %w)\n","  print_neighbors(euclidean, vocab_5k, W2VEmbeddings, 'good')\n","  print_neighbors(cosine, vocab_5k, W2VEmbeddings, 'good')"],"execution_count":33,"outputs":[{"output_type":"stream","text":["----------WINDOW 1 ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['well', 'time', 'any', 'too', 'much', 'after', 'would', 'also', 'than']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['that', 'who', 'and', 'to', 'i', 'with', 'or', 'an', 'of']]\n","----------WINDOW 2 ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['also', 'while', 'character', 'most', 'get', 'films', 'me', 'plot', 'off']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['who', 'that', 'and', 'or', 'a', 'with', 'an', 'to', 'when']]\n","----------WINDOW 3 ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['two', 'character', 'films', 'well', 'time', 'much', 'would', 'also', 'after']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['an', 'and', 'who', 'a', 'that', 'with', 'to', 'when', 'of']]\n","----------WINDOW 5 ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['character', 'also', 'get', 'we', 'would', 'after', 'time', 'been', 'than']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['who', 'and', 'with', 'an', 'or', 'of', 'to', 'a', 'from']]\n","----------WINDOW 10 ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['we', 'him', 'time', 'been', 'can', 'there', 'than', 'any', 'only']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['that', 'who', 'with', 'a', 'an', 'of', 'to', 'or', 'as']]\n","----------WINDOW 15 ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['than', 'can', 'there', 'we', 'him', 'get', 'after', 'any', 'only']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['who', 'with', 'an', 'of', 'to', 'that', 'as', 'or', 'a']]\n","----------WINDOW 20 ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['get', 'story', 'two', 'than', 'there', 'character', 'off', 'any', 'after']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['an', 'that', 'and', 'with', 'who', 'to', 'from', 'of', 'as']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n4Xz8wYKFJmD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"48f305c6-dce7-436f-b2d4-b4f460d6f25a","executionInfo":{"status":"ok","timestamp":1583810058213,"user_tz":-60,"elapsed":245133,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["iters=[300,500,1000,5000]\n","for i in iters:\n","  model = Word2Vec(size=300,\n","                 window=5,\n","                 null_word=1,\n","                 iter=i)\n","  model.build_vocab_from_freq(word_counts_5k)\n","  model.train(preprocess_generator(texts[:]), total_examples=10, epochs=30, report_delay=1)\n","  W2VEmbeddings = model.wv.vectors\n","  print('----------%i ITERATIONS ---------' %i)\n","  print_neighbors(euclidean, vocab_5k, W2VEmbeddings, 'good')\n","  print_neighbors(cosine, vocab_5k, W2VEmbeddings, 'good')"],"execution_count":34,"outputs":[{"output_type":"stream","text":["----------300 ITERATIONS ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['also', 'any', 'than', 'character', 'after', 'time', 'two', 'can', 'films']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['who', 'that', 'and', 'an', 'with', 'to', 'a', 'of', 'or']]\n","----------500 ITERATIONS ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['time', 'can', 'than', 'much', 'any', 'get', 'after', 'also', 'character']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['who', 'that', 'and', 'an', 'with', 'a', 'to', 'of', 'or']]\n","----------1000 ITERATIONS ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['also', 'time', 'character', 'two', 'would', 'any', 'get', 'after', 'films']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['who', 'and', 'that', 'a', 'with', 'an', 'or', 'to', 'from']]\n","----------5000 ITERATIONS ---------\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['will', 'also', 'time', 'two', 'character', 'would', 'any', 'get', 'after']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['who', 'and', 'that', 'an', 'a', 'or', 'to', 'from', 'with']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7NNzy0_bHA41"},"source":["## Application à l'analyse de sentiments\n","\n","On va maintenant utiliser ces représentations pour l'analyse de sentiments. \n","Le modèle de base, comme dans le TP précédent, sera construit en deux étapes:\n","- Une fonction permettant d'obtenir des représentations vectorielles des critiques, à partir des textes, du vocabulaire, et des représentations vectorielles des mots. Une telle fonction (à compléter ci-dessous) va associer à chaque mot d'une critique son embeddings, et créer la représentation pour l'ensemble de la phrase en sommant ces embeddings.\n","- Un classifieur qui prendra ces représentations en entrée et réalisera une prédiction. Pour le réaliser, on pourra utiliser d'abord la régression logistique ```LogisticRegression``` de ```scikit-learn```  "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UAzQUoRaHA41","colab":{}},"source":["def sentence_representations(texts, vocabulary, embeddings, np_func=np.sum):\n","    \"\"\"\n","    Represent the sentences as a combination of the vector of its words.\n","    Parameters\n","    ----------\n","    texts : a list of sentences   \n","    vocabulary : dict\n","        From words to indexes of vector.\n","    embeddings : Matrix containing word representations\n","    np_func : function (default: np.sum)\n","        A numpy matrix operation that can be applied columnwise, \n","        like `np.mean`, `np.sum`, or `np.prod`. \n","    Returns\n","    -------\n","    np.array, dimension `(len(texts), embeddings.shape[1])`            \n","    \"\"\"\n","    #\n","    # A compléter !\n","    # \n","    # Associer à chaque mot d'une critique son embeddings\n","    representations=np.zeros((np.shape(texts)[0],np.shape(embeddings)[1])) #vecteur avec en ligne les texts, et en colonne la taille embedding\n","    for i,text in enumerate(texts):\n","        text=clean_and_tokenize(text)\n","        rep=[]\n","        for mot in text:\n","            #verifier si mot dans vocabulaire ! 'boulevard' n'y est pas\n","            if mot in vocabulary:\n","                idx=vocabulary[mot]\n","            else:\n","                idx=vocabulary['UNK']\n","            rep.append(embeddings[idx,:]) #on rassemble tous les embeddings des mots\n","        rep=np.sum(rep,axis=0) #representation de la phrase\n","        representations[i,:]=rep\n","    \n","    #Créer la représentation pour l'ensemble de la phrase en sommant ces embeddings\n","    \n","    return representations"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"V21TVXZzHA44","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"9633761c-b950-4552-97b8-bfe6acf53040","executionInfo":{"status":"ok","timestamp":1583810088029,"user_tz":-60,"elapsed":274903,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","\n","# Exemple avec les embeddings obtenus via Glove\n","rep = sentence_representations(texts, vocab_5k, GloveEmbeddings)\n","\n","clf = LogisticRegression(max_iter=10000).fit(rep[::2], y[::2])\n","print(clf.score(rep[1::2], y[1::2]))\n","\n","scores = cross_val_score(clf, rep, y, cv=5)\n","print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))\n"],"execution_count":36,"outputs":[{"output_type":"stream","text":["0.749\n","Score de classification: 0.7875 (std 0.015890248582070717)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FsTs1CqJSkk6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"9fcf00be-3e9c-4b68-d043-59550fce4884","executionInfo":{"status":"ok","timestamp":1583810106453,"user_tz":-60,"elapsed":293306,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["vocab_5k_2, word_counts_5k_2 = vocabulary(texts, 2000, 5000)\n","rep = sentence_representations(texts, vocab_5k_2, GloveEmbeddings)\n","\n","clf = LogisticRegression(max_iter=10000).fit(rep[::2], y[::2])\n","print(clf.score(rep[1::2], y[1::2]))\n","\n","scores = cross_val_score(clf, rep, y, cv=5)\n","print('Score de classification: %s (std %s)' % (np.mean(scores), np.std(scores)))\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["0.686\n","Score de classification: 0.698 (std 0.0207605394920267)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6ff0LVrI8Kla","colab_type":"text"},"source":["En utilisant la regression logistique (modèle discriminatif) pour prédire les avis (positif ou négatif) des critiques de films, on obtient un score de **0.7875**. En prenant en compte que dans l'état de l'art (https://arxiv.org/pdf/1711.08609.pdf) pour du \"movie review\" le score sur Gl0ve est d'environ 0.792, ce résultat est plutôt satisfaisant. Néanmoins, il existe maintenant d'autres modèles qui permettent d'augmenter cette précision. "]},{"cell_type":"markdown","metadata":{"id":"vqKU5g9tTek1","colab_type":"text"},"source":["Je me suis aussi intérésée à l'influence du parametre \"count_threshold\" qui permet de garder que les mots qui apparaissent un certain nombre de fois dans le vocabulaire. Je pensais qu'en augmentant ce seuil les mots qui seraient considérés comme du \"bruit\" (trop peu fréquents) seraient enlevés et donc la prédiction serait meilleure, mais ce n'est pas le cas. Avec un seuil à 2500 on obtient un score de **0.686** qui est nettement inférieur à celui d'avant.\n","\n","Ceci peut être dû au fait que certains mots, même s'ils ne sont pas fréquents, expriment bien les sentiments des personnes donnant leur avis."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0TOWx8Z-HA49"},"source":["Vous pouvez maintenant comparer l'ensemble des méthodes, et notamment répondre aux questions suivantes:\n","- Pourquoi peut-on s'attendre à ce que les résultats obtenus avec les embeddings extraits des représentations pré-apprises avec Gl0ve soient bien meilleurs que les autres ? Quel serait le moyen de comparer de manière 'juste' Gl0ve avec les autres méthodes d'apprentissage de représentations ?\n","- Quelle matrice permet d'obtenir les meilleures représentations via SVD ? (Co-occurences, Tf-Idf, PPMI ..)\n","- Word2Vec est difficile à paramétrer. Essayez d'améliorer les représentations en changeant la taille du contexte, le nombre d'itérations ..."]},{"cell_type":"markdown","metadata":{"id":"L_SD461rxPnX","colab_type":"text"},"source":["### 1) Pourquoi peut-on s'attendre à ce que les résultats obtenus avec les embeddings extraits des représentations pré-apprises avec Gl0ve soient bien meilleurs que les autres ? Quel serait le moyen de comparer de manière 'juste' Gl0ve avec les autres méthodes d'apprentissage de représentations ?\n","\n","On peut s'attendre à ce que les résultats obtenus avec Gl0ve soit meilleurs que les autres parce que ils sont extraits d'un pre-entrainnement, qui a été fait sur un corpus beaucoup plus grand que le notre. D'après la documentation, les embeddings obtenus par Gl0ve ont été entraînnés sur des données de Wikipedia et Gigaword 5 (données d'actualités). Le corpus d'entraînnement est donc beaucoup plus grand et plus varié : les résultats sont donc plus riches que ce que nous pourront obtenir en comptant les occurances de \"seulement\" 2000 documents. "]},{"cell_type":"markdown","metadata":{"id":"t8VDD3RV1STy","colab_type":"text"},"source":["### Quelle matrice permet d'obtenir les meilleures représentations via SVD ? (Co-occurences, Tf-Idf, PPMI ..)\n","\n","La matrice qui permet d'avoir des meilleures représentations via SVD est la matrice de **co-occurences**. Comme nous avons vu avant, c'est avec cette matrice que nous trouvions des voisins de \"good\" plus \"cohérents\" (cf résultats et commentaires plus haut). Bien-sur, on juge la qualité des mots trouvés de façon un peu subjective, mais globalement on peut dire que les meilleures représentations sont obtenues aveec la matrice de co-occurence pour une fenêtre petite de 5, et lorsque la distance cosinus est utilisée. \n","\n","Comme on pouvait s'en douter, les représentations semblent plus cohérentes avec M5dist qu'avec PPMI5 ou TFIDF5 lorsque l'on utilise SVD:\n","\n","M5dist\n","Plus proches voisins de good selon la distance 'cosine': \n","[['very', 'bad', 'plot', 'funny', 'simple', 'pace', 'great', 'fine', 'shock']]\n","\n","PPMI5\n","Plus proches voisins de good selon la distance 'cosine': \n","[['bad', 'but', 'this', 'some', 'very', 'movie', 'it', 'its', 'that']]\n","\n","TFIDF\n","Plus proches voisins de good selon la distance 'cosine': \n","[['supposed', 'very', 'moments', 'it', 'completely', 'doesnt', 'is', 'too', 'plausible']]"]},{"cell_type":"markdown","metadata":{"id":"77swKr_k1YLn","colab_type":"text"},"source":["### Word2Vec est difficile à paramétrer. Essayez d'améliorer les représentations en changeant la taille du contexte, le nombre d'itérations ...\n","\n","En essayant de jouer sur les paramètres, nous constatons plusieurs choses:\n","\n","*   Même si augmenter ou diminuer la taille de la fenêtre n'aboutit pas à des grandes améliorations, plus la taille est grande, plus les mots voisins tendent à être des verbes ou des propositions. Par exemple, pour une fenêtre 2 nous avons ('characters', 'plot', 'while', 'character', 'director', 'too', 'two', 'also', 'me') plusieurs mots relatifs au cinéma, alors que pour une grande taille de 20 ('off', 'get', 'other', 'there', 'story', 'than', 'been', 'make', 'any') il n'y en a qu'un seul. Ceci semble logique dans le sens ou plus la fenêtre est grande, plus il y aura des mots à prendre en compte, et donc normalement plus de verbes et prépositions (qui sont des mots très communs).\n","*   Le nombre d'itérations effectuées ne semble pas changer la qualité du résultat dans ce cas.\n","* La distance cosinus trouve toujours des mots très communs ('a', 'that', 'for', 'the', 'and', 'to', 'is', 'on', 'of'), et donc ne donne pas des voisins pertinents de \"good:.\n","\n"]},{"cell_type":"code","metadata":{"id":"hNNHt1W4xWYv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":578},"outputId":"2c5ee5fe-5ec3-40c2-cd3a-302281aee64f","executionInfo":{"status":"ok","timestamp":1583810223539,"user_tz":-60,"elapsed":410369,"user":{"displayName":"Aloibaf Azonipse","photoUrl":"","userId":"01440395522638646397"}}},"source":["#petit test pour voir l'influence tu paramètre count_threshold\n","\n","thres=[0,100,1000]\n","for t in thres:\n","  vocab_5k, word_counts_5k = vocabulary(texts, t, 5000)\n","  M5dist = co_occurence_matrix(texts, vocab_5k, window=5, distance_weighting=True)\n","  M20 = co_occurence_matrix(texts, vocab_5k, window=20, distance_weighting=False)\n","  print('------ threshold %i ------'%t)\n","\n","  print(\"Avec un contexte large, sans prendre en compte la distance entre les mots:\")    \n","  print_neighbors(euclidean, vocab_5k, M20, 'good')\n","  print_neighbors(cosine, vocab_5k, M20, 'good')\n","  print(\"Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\")    \n","  print_neighbors(euclidean, vocab_5k, M5dist, 'good')\n","  print_neighbors(cosine, vocab_5k, M5dist, 'good') "],"execution_count":38,"outputs":[{"output_type":"stream","text":["------ threshold 0 ------\n","Avec un contexte large, sans prendre en compte la distance entre les mots:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['story', 'can', 'much', 'also', 'will', 'time', 'character', 'we', 'films']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['bad', 'very', 'great', 'really', 'not', 'quite', 'it', 'fun', 'completely']]\n","Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['bad', 'very', 'plot', 'comedy', 'just', 'little', 'there', 'man', 'had']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['very', 'bad', 'plot', 'great', 'not', 'its', 'funny', 'it', 'movie']]\n","------ threshold 100 ------\n","Avec un contexte large, sans prendre en compte la distance entre les mots:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['story', 'can', 'also', 'will', 'we', 'time', 'films', 'two', 'character']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['bad', 'very', 'great', 'not', 'really', 'it', 'fun', 'quite', 'just']]\n","Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['bad', 'very', 'plot', 'there', 'had', 'man', 'little', 'life', 'time']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['very', 'bad', 'plot', 'great', 'not', 'funny', 'little', 'job', 'fun']]\n","------ threshold 1000 ------\n","Avec un contexte large, sans prendre en compte la distance entre les mots:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['we', 'time', 'most', 'can', 'will', 'no', 'only', 'story', 'than']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['bad', 'very', 'great', 'not', 'little', 'really', 'just', 'it', 'here']]\n","Avec un contexte plus petit, et en réduisant l'impact des paires de mots selon leur distance:\n","Plus proches voisins de good selon la distance 'euclidean': \n","[['time', 'no', 'story', 'than', 'my', 'there', 'new', 'two', 's']]\n","Plus proches voisins de good selon la distance 'cosine': \n","[['very', 'great', 'little', 'bad', 'plot', 'not', 'while', 'its', 'like']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jYUJhXaLQIr9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}